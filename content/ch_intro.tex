\chapter{绪论}
\section{研究背景和意义}
自然语言处理（NLP）主要分为词法分析、句法分析和语义分析三个层面.
其中句法分析旨在分析输入句子的句法结构，将句子从词语序列转化为树状结构，能更好地刻画远距离依赖关系.
句法分析作为连接词法分析和语义分析的中间环节，既可以用来消解词法分析中常见的词法歧义问题如一词多词性，又可以用来帮助语义分析以及其他NLP的下游任务如关系抽取，机器翻译，问答系统等.
因此句法分析具有十分重要的理论意义和应用价值.

目前研究界主流的句法体系有短语结构句法体系\upcite{collobert2011natural}和依存结构句法体系\upcite{mel1988dependency}. 短语结构句法体系以获取短语结构与短语之间的层次句法关系为目的，而依存结构句法体系旨在识别词语之间的修饰关系. 依存句法分析具有灵活，句法结构扁平，易于理解和应用等优点\upcite{马金山2007基于统计方法的汉语依存句法分析研究}，因此逐渐得到研究者的关注和采用. 目前依存结构体系广泛地应用于不同语言中，研究者们提出了很多不同的依存句法分析方法，主要分为基于转移的句法分析方法和基于图的句法分析方法. 基于转移的方法将句法树的生成建模成寻找最优动作序列的过程，而基于图的方法的思想是从有向完全图中寻找一棵最大生成树.

近年来，随着深度学习的发展，基于神经网络的依存句法分析模型取得了长足的进步，准确率大大超过了基于传统离散特征的句法分析模型\upcite{chen2014fast,dyer2015transition,zhou2015neural,andor2016globally,dozat2017deep}. 尤其是，Dozat和Manning（2017）\upcite{dozat2017deep}提出了一种简单有效的基于双仿射深层神经网络的依存句法分析器（本文称之为BiaffineParser），在很多语言和数据集上取得了目前最好的句法分析准确率.

然而，无论是基于传统离散特征的句法分析方法还是基于神经网络的句法分析方法，本质上大都是有监督的机器学习方法. 这意味着树库标注规模的大小往往直接影响着句法分析的准确率.
不幸的是，构建一个大规模的依存树库需要投入大量的人工成本，其中涉及到很多艰巨的任务：标注规范的制定，标注人员的培训，标注质量的控制等等. 因此，虽然很多语言同时存在着多个遵守不同标注规范或语言学理论的句法树库（称之为异构树库），但是这些树库的规模和涵盖的领域都相当有限.

考虑到上述情况，只使用单个句法树库资源似乎无法进一步提升句法模型性能了\upcite{zhli2013doctor}. 一方面，研究者开始尝试使用其他资源来帮助依存句法分析任务，如大规模的未标注数据\upcite{koo2008simple,chen2009improving,bansal2011web}，双语对齐文本\upcite{huang2009bilingually,chen2010bitext}等等. 另一方面，同一语言的多源异构树库都是为了刻画该语言的句法结构而标注的，其中包含了很多共同的语言现象. 如何利用多源异构数据中存在的共同的语法知识来提高句法分析的准确率，一直是研究界关注的方向. 由于缺乏双树对齐数据，即一个句子存在两种不同规范的句法树，前人所提出的利用异构树库的方法主要可以分为无监督的树库转化方法\upcite{magerman1994natural,nivre2004deterministic,niu2009exploiting,zhu2011better,li2013iterative}和间接的树库融合方法\upcite{li2012exploiting,guo2016universal,stymne2018parser}. 其中，树库转化是一个非常自然的想法，其目的是将源端树库转化为符合目标端规范的树库，从而扩大目标端句法树库的规模，进而提升目标端句法模型的性能.
%研究表明树库转化是一种简单有效的利用异构树库的方式.

本文二、三两章关注于利用异构树库来提升目标端句法模型性能. 我们通过标注少量双树对齐数据，提出了有监督的树库转化任务；然后设计并实现了两种性能相当的树库转化模型；最后通过利用转化后树库，显著提高了目标端依存句法分析的准确率. 随后，我们改进了树库转化方法和树库融合方法，提升了转化模型的速度和性能，并最终提升了目标端句法模型的性能.

此外，依存句法分析的输出结果刻画了词语间的修饰关系，表示了人类对文本理解的结构信息，有利于更好地理解输入的句子.
研究者尝试将依存句法应用到了NLP下游任务中，许多工作表明了句法的有效性\upcite{marcheggiani2017encoding,tai2015improved,guo2019attention,bastings2017graph}.
因此，依存句法的应用是另一个深受学术界关注的方向.
一方面，研究者探索了大量的句法应用场景，如语义分析，关系抽取，机器翻译等等. 但是还有很多的NLP的下游任务待被探索.
另一方面，句法应用到下游任务时，不可避免地会受到错误传递问题的影响，如何合理地使用依存句法信息仍然具有挑战性.
因为 1）虽然目前句法分析取得了长足的进步，但是即使是目前最好的句法模型仍然会产生大量的错误分析，尤其是在处理长文本时；
2）句法分析模型有很强的领域相关性，当跨领域时，句法分析准确率会大幅下降.
因此，进一步探索句法的应用场景和应用方式是一项充满挑战的研究.

针对依存句法的应用问题，本文第四章全面探讨了三种形式的依存句法信息，提出并尝试了多种方法将这些句法信息应用到基于双向LSTM-CRF的意见角色标注（Opinion Role Labeling, ORL）任务上，显著地提升了ORL模型的识别能力.
%本文，我们在
%场景
%性能好，错误
%领域
%充满挑战
%由于缺乏双树对齐数据，即一个句子存在两种不同规范的句法树，前人所提出的利用异构树库的方法主要可以分为无监督树库转化方法和间接树库融合方法.

%因此，虽然很多语言同时存在着多个遵守不同标注规范或语言学理论的句法树库（如表格xx所示，称之为异构树库），
%但是这些树库的规模和涵盖的领域都相当有限.
%%依存句法分析是一种结构化预测的问题，比二元分类和序列标注问题更具挑战性，因此依存句法模型更容易受到数据稀疏问题的影响，树库规模很大程度上影响着依存句法模型的准确率.
%%然而，构建一个一定规模的依存树库需要投入大量的人工成本，其中涉及到很多艰巨的任务：标注规范的制定，标注人员的培训，标注质量的控制等等.
%%因此，虽然很多语言同时存在着多个遵守不同标注规范或语言学理论的句法树库（如表格xx所示，称之为异构树库），
%%但是这些树库的规模和涵盖的领域都相当有限.
%%值得注意的是，虽然异构树库遵循着不同的标注规范，但是他们之间存在着很多相同的标注结构，体现了不同规范之间的共性.
%%如何利用多源异构数据提高句法分析性能，一直是研究界关注的方向.
%由于缺乏双树对齐数据，即一个句子存在两种不同规范的句法树（如图1所示），
%前人所提出的利用异构树库的方法主要可以分为无监督树库转化方法
%(Magerman et al., 1994[6]; Nivre et al., 2004[7]; Niu et al., 2009[8]; Zhu et al., 2011[9]; Li et al., 2013[10])
%和间接树库融合方法(Li et al., 2012[11]; Guo et al., 2016[12]; Stymne et al., 2018[13]).

%引入依存句法，句法重要性
%解决方法，数据驱动的模型 需要数据，异构数据 树库转化放啊
% 一方面提示句法准确率，另一方面，我们要合理利用句法信息，提升下游任务的性能

\section{研究现状}

\subsection{依存句法分析}
%\textbf{任务定义.  }
依存语法的现代理论由法国语言学家Lucien Tesni\'{e}re于十九世纪五十年代末发表\upcite{tesniere2015elements}，他认为依存关系是词语之间的支配关系，可以用来表示语言结构.
Hays和Gaifman \upcite{hays1964dependency}最早开始采用依存句法表示句子结构的研究. 目前，依存句法分析将句子的词语序列转化为表示词语间修饰关系的树状结构. 我们通常把该树状结构称为依存句法树，它的核心单位是一条从核心词指向修饰词，并带有依存关系的依存弧. 其中依存关系可以细分为多种不同的类型，用以表示两个词语之间的句法或语义关系，如主谓关系，并列关系等.

\begin{figure*}[hb!]
    %\vspace{-0.5em}
    \centering
    \includegraphics[angle=0,width=0.6\textwidth]{img/example-dependency-crop.pdf}
    \caption{依存句法树示例. }
    \label{fig:example_dependency}
    %\vspace{-1.5em}
\end{figure*}
\textbf{任务定义.  }给定句子词序列$S=w_1w_2\dots w_n$和词性序列$T=t_1t_2\dots t_n$（由于词性序列构成了丰富的特征，往往作为依存句法分析的额外输入），依存句法分析输出一棵句法树$d=\{(h,m,l)|0 \leq h \leq n, 0 \textless m \leq n, l \in L\}$，其中$(h,m,l)$表示由核心词$w_h$指向修饰词$w_m$的依存弧$m \leftarrow h$，依存关系是$l$，$L$是依存关系标签集合. 图\ref{fig:example_dependency}给出了一棵依存句法树的例子. 其中由“叫”指向“奶奶”的依存弧“$\mbox{奶奶} \leftarrow \mbox{叫}$”表示“奶奶”修饰“叫”，并且依存关系是“SBV”（主语关系）. 此外，“$\$$”是一个伪词，指向整个句子的核心词，简化了依存句法分析的形式化定义.
%\textbf{评价指标.  }

由于易于标注和应用等优点，依存句法分析被广泛研究，并被多种语言所采用. CoNLL国际会议（Conference on Computational Natural Language Learning）\upcite{buchholz2006conll,nivre2007conll,surdeanu2008conll,hajivc2009conll}先后多次举办了依存句法分析的评测比赛；Google也曾与2012年组织了SANCL评测，旨在利用大规模的网络无标注数据提升依存句法模型性能. 这些评测比赛为国内外的研究人员提供了多种语言的标注数据和集中学习交流的平台.
%给定句子词序列$S=w_1, w_2, \dots, w_n$和词性序列$T=t_1, t_2, \dots, t_n$，依存句法分析输出一棵句法树$d=\{(h,m,l)|0 \leq h \leq n, 0 \le m \leq n\}$，其中$(h,m,l)$表示由核心词$w_h$指向修饰词$w_m$的依存弧，依存关系是$l$.

研究者提出了多种依存句法分析算法，主流的有基于转移的分析方法和基于图的分析方法.
基于转移的依存句法分析方法由Nivre等（2006）\upcite{nivre2006labeled}提出，他将句法树的构建建模成寻找最优动作序列的过程：在预先定义的转移系统下（包括有限个转移动作，一个堆结构，一个栈结构），从左至右遍历句子，以贪心搜索（gready search）或者柱搜索（beam search）的方式逐步解码，复杂度为线性时间$O(2n+1)$（$n$为句子词数）.
随后研究者针对“语言中的长距离依赖”\upcite{ren2009parsing}，“无法获取全局特征”\upcite{zhang2011transition}，“贪心解码算法造成的错误传递”\upcite{buckman2016transition}等问题提出了解决方法，大大提升了基于转移的依存句法分析方法的准确率.
基于图的依存句法分析方法由McDonald等（2005）\upcite{mcdonald2005online}首次提出，他将句法树的构建建模为从有向完全图中搜索一棵最大生成树的过程：任意两词间采用有向边连接，构成完全有向图，以全局解码的方式搜素最优结果. 随后研究者考虑到兄弟节点，祖孙节点之间的相关性\upcite{mcdonald2006online,carreras2007experiments,koo2010efficient}，引入了高阶句法特征，使得模型的分析结果更加合理.
% ,
两种方法各有优缺点，基于图的分析方法准确率高但解码复杂度高，基于转移的分析方法能线性时间内解码但是准确率低. 研究者尝试融合两种算法，达到相互弥补的目的. McDonald和Nivre（2008）\upcite{nivre2008integrating}采用MSTParser（基于图的句法分析器）和MALTParser（基于转移的句法分析器）为案例，通过将一个句法器产生的特征融入到另一个句法器中提出依存句法分析综合模型. Bohnet等人（2012）\upcite{bohnet2012best}尝试利用平行算法和哈希核函数，来进一步提升模型的分析性能和速度.

随着深度学习的发展，许多工作表明了基于神经网络的依存句法分析模型能明显提升依存句法分析的准确率.
Chen和Manning（2004）\upcite{chen2014fast}首次实现了神经网络框架下的基于转移的分析方法. 他们设计了一个输入特征模板，并仅通过简单的前馈神经网络层训练了一个非线性的转移动作分类器. %，明显提升了句法分析的准确率.
在此基础上，Weiss等（2015）\upcite{weiss2015structured}提出结构化训练方法，并采用残差网络（Residual Network）和柱搜索进一步提升模型的性能.
Dyder等（2015）\upcite{dyer2015transition}提出使用长短期记忆神经网络（LSTM）来记录转移系统中栈结构，堆结构和已有动作序列的信息，既避免了人工定义输入模板的繁琐又解决了长距离的依赖问题.
%syntaxNet.
Kiperwasser等（2016）\upcite{kiperwasser2016simple}使用双向LSTM（双向LSTM）作为句子编码器，用于抽取句子的文本信息，实现了基于图的分析方法.
%Hashimoto等通过注意力机制计算图中的每条弧的分数.
在此基础上，Dozat和Manning（2017）\upcite{dozat2017deep}提出简单有效的基于深度双仿射（Biaffine）注意力机制的句法模型，在多个数据集上达到了目前最好的性能.

\subsection{多源异构树库的融合}
考虑到树库构建的高成本，利用各种异构树库来提高句法模型的分析准确率，一直是一个有趣而有吸引力的研究方向. 如表\ref{tb:treebanks}所示，我们列出了规模较大的中文树库，其中包括了短语结构树库和依存结构树库. 研究者尝试利用这些多源异构树库中存在的共同的语法现象来提升目标端句法模型的性能.
%研究者尝试利用额外的短语结构树库来帮助短语/依存句法分析模型\upcite{}，也尝试了利用额外的依存树库来帮助短语/依存句法分析模型\upcite{}.
%\begin{table}[tb]
%	%\begin{footnotesize}
%	\centering
%		\begin{tabular}{c c c}
%			\hline
%			Treebanks & \#Tok & Grammar \\%& Source \\
%			\hline
%			Sinica \cite{sinica-03} & 0.36M  & Case grammar \\ %(traditional Chinese) \\
%			CTB \cite{XueXiaChiouPalmer2005} & 1.62M  & Phrase structure \\
%			TCT \cite{zhouqiang-04-TCT} & 1.00M & Phrase structure \\
%			PCT \cite{zhan-weidong-12-treebank} & 0.90M  & Phrase structure \\
%			HIT-CDT \cite{che-li-12-cdt} & 0.90M & Dependency structure \\
%			PKU-CDT \cite{qiu-likun-c14-treebank} & 1.40M & Dependency structure \\
%			\hline
%		\end{tabular}
%		\caption{Large-scale Chinese treebanks (token number in million). %, scale in millions.
%		}
%		\label{tb:treebanks}
%	%\end{footnotesize}
%\end{table}

\begin{table}[tb]
    %\begin{footnotesize}
    \centering
    \begin{tabular}{c c c c}
        \toprule
        树库    & 词数  & 句法体系 & 语料来源                 \\%& Source \\
        \hline
        %Sinica & 0.36M  & Case grammar \\ %(traditional Chinese) \\
        CTB     & 1.62M & 短语结构 & 新闻、广播、访谈         \\
        TCT     & 1.00M & 短语结构 & 新闻、文学、             \\
        PCT     & 0.90M & 短语结构 & 新闻、政府文书、语文课本 \\
        \hline
        HIT-CDT & 1.11M & 依存结构 & 人民日报                 \\
        PKU-CDT & 1.40M & 依存结构 & 新闻、专利、医药         \\
        \bottomrule
    \end{tabular}
    \caption{大规模的中文树库. %, scale in millions.
    }
    \label{tb:treebanks}
    %\end{footnotesize}
\end{table}

总体来说，研究者主要采用了间接的树库融合方法和直接的树库转化方法来利用异构树库提升目标端模型性能.
\begin{enumerate}
    \item 间接的树库融合方法
          
          Li等（2012）\upcite{li2012exploiting}提出基于准同步文法的多树库融合方法. 他们首先用训练好的源端句法模型解析目标端树库，构造了伪双树对齐数据；然后利用模式刻画源端结构到目标端结构的对应关系；最后在模式的基础上提出QG特征来将源端树库蕴含的知识融入到基于传统离散特征的目标端句法模型中，指导目标端树的构建. Guo等（2016）\upcite{guo2016universal}首次采用多任务学习框架利用异构树库. 他们把多个异构树库句法模型视为不同的任务同时训练，通过共享编码层参数学习树库间的共同句法信息，通过独立参数来区分每个树库的独特之处. Stymne等（2018）\upcite{stymne2018parser}提出一种简单灵活且有效的方法来利用多个UD树库（Universal Dependencies Treebanks，多达25种语言不止拥有一个树库）. 他们提出使用树库嵌入向量（treebank embedding）来区分属于同一语言的不同的树库，非常简单地训练出了能适应多个异构树库的句法模型，且达到了很好的性能.
          
          
          %首先，我们给出树库转化的\textbf{任务定义}：树库转化旨在将源端句法标注结构转化为目标端句法标注结构，如图\ref{fig:example-su-vs-hit-cdt}所示，就是将下方的源端结构转化为上方的目标端结构.
          %形式化，
          %给定句子的词序列$S=w_1, w_2, \dots, w_n$，词性序列$T=t_1, t_2, \dots, t_n$
          %和源端句法树$d^{\texttt{src}}=\{(h,m,l) | 0 \leq h \leq n, 0 \textless m \leq n, l \in L^{\texttt{src}}\}$，
          %树库转化任务输出一棵符合目标端规范的句法树$d^{\texttt{tgt}}=\{(h,m,l)|0 \leq h \leq n, 0 \textless m \leq n, l \in L^{\texttt{tgt}} \}$，
          %其中$(h,m,l)$表示由核心词$w_h$指向修饰词$w_m$的依存弧$ m \leftarrow h$，依存关系是$l$，$L^{\texttt{src}}$是源端规范依存关系的集合，$L^{\texttt{tgt}}$是目标端规范依存关系的集合.
          %借助树库转化任务，源端树库被转化为目标端树库，从而扩大了目标端树库的规模.
          
    \item 直接的树库转化方法
          
          \textbf{任务定义.  }树库转化旨在将源端句法标注结构转化为目标端句法标注结构. 和依存句法分析一样，树库转化的输出也是一棵句法树，不同的是，树库转化的输入多了一棵源端句法树. 形式化，给定句子的词序列$S=w_1w_2\dots w_n$，词性序列$T=t_1t_2\dots t_n$
          和源端句法树$d^{\texttt{src}}=\{(h,m,l) | 0 \leq h \leq n, 0 \textless m \leq n, l \in L^{\texttt{src}}\}$，
          树库转化任务输出一棵符合目标端规范的句法树$d^{\texttt{tgt}}=\{(h,m,l)|0 \leq h \leq n, 0 \textless m \leq n, l \in L^{\texttt{tgt}} \}$，
          其中$(h,m,l)$表示由核心词$w_h$指向修饰词$w_m$的依存弧$ m \leftarrow h$，依存关系是$l$，$L^{\texttt{src}}$是源端规范依存关系的集合，$L^{\texttt{tgt}}$是目标端规范依存关系的集合.
          
          间接的树库融合方法要么不能直接利用源端数据作为训练数据，要么不能直接刻画源端结构和目标端结构的对应关系. 而直接的树库转化方法旨在将源端树库转化为符合目标端规范的树库，从而扩大目标端句法树库规模，进而提升目标端句法模型性能.
          
          %\textbf{任务定义}：树库转化旨在将源端句法标注结构转化为目标端句法标注结构，相较与
          
          %由于缺少双树对齐数据，前人往往关注于无监督的树库转化方法.
          由于源端树库和目标端树库不重叠，前人往往关注于无监督的树库转化方法.
          Magerman等（1994）\upcite{magerman1994natural}提出了核心节点映射表，通过优先序列来确定一个组块中的核心节点；
          Nivre等（2004）\upcite{nivre2004deterministic}定义了一套启发式规则来确定弧的依存关系类型.
          Niu等（2009）\upcite{niu2009exploiting}综合考虑模型概率和目标端树库与源端树库的一致率，对模型产生的n-best tree结果进行重排序.
          Zhu等（2011）\upcite{zhu2011better}通过源端句法模型构造伪双树对齐数据，从而训练基于特征表示的转化模型.
          Li等（2013）\upcite{li2013iterative}提出一种迭代训练算法来逐步提高伪双树对齐数据质量，进而提高树库融合性能.
          
          据我们所知，目前有两个工作探索了有监督的树库转化方法，分别是我们在ACL-2018会议和NLPCC-2019会议上发表的工作，也是本文二，三两章的主要研究内容. 我们通过构建少量的双树对齐数据，提出了有监督的树库转化任务，并设计实现了两种性能相当的转化模型. 之后，我们提出了更高效的树库转化方法，并更合理地使用了转化后树库，进一步提升了目标端模型的性能.
          
\end{enumerate}

\subsection{依存句法的应用}
%分开后，这一节太少了，按照任务来找相关工作，SRL，MT，RE.
依存句法分析输出的树状结构表明了句子中词语之间的修饰关系，能更好地刻画远距离依赖关系.
大量工作研究了如何编码依存句法树的信息，并成功地应用到了许多自然语言处理任务中.

其中，依存句法常被用于帮助上层的语义分析任务，尤其是语义角色标注任务（Semantic Role Labeling，SRL）. 研究者采用不同方法编码依存句法树信息，提出了大量句法感知（Syntax-aware）的SRL模型.
Marcheggiani和Titov（2017）\upcite{marcheggiani2017encoding}采用GCN（Graph Convolutional Networks）将句法信息融入到SRL模型中.
Roth和Lapata（2016）\upcite{roth2016neural}利用LSTM（Long Short-Term Memory）模型来获得依存路径的向量表示，而Xia等（2019）\upcite{xia2019syntax}利用谓词与其他词之间的相对位置来表示依存关系路径.
Marcheggiani等（2017）\upcite{marcheggiani2017simple}提出使用句法感知的预训练词向量表示句法信息，而Xia等（2019）\upcite{xia2019mtl}在多任务学习的框架下，将句法感知的预训练词向量引入到SRL模型中. %（句法模型LSTM的隐状态）

此外，依存句法信息的引入也能有效提升其他NLP上层任务的性能.
Tai等（2015）\upcite{tai2015improved}提出树状LSTM（Tree-structured Long Short-term Memory）来全局编码句法树信息，句法信息的融入改善了语义相关性任务和情感分析任务的性能；
%Kipf和Welling\upcite{kipf2016semi}提出了图卷积神经网络（Graph Convolutional Networks，GCN）编码图结构的数据.
Bastings等（2017）\upcite{bastings2017graph}根据依存句法树的方向性以及依存关系对GCN进行扩展，将句法信息引入到了基于Encoder-Decoder的机器翻译任务中.
Guo等（2019）\upcite{guo2019attention}采用自注意力机制将句法树转化为句法森林，然后采用GCN将句法森林融入到关系抽取任务中.

然而，据我们所知，除了Yang和Cardie（2013）\upcite{yang-cardie-2013-joint}通过模式将依存句法信息引入到意见角色标注任务（Opinion Role Labeling，ORL）中，目前没有其他相关工作探索依存句法在ORL任务中的应用. ORL是一种细粒度的意见分析任务，旨在识别出句子中意见的实体. 如图\ref{fig:example_orl}所示，意见实体由<意见措辞-意见持有者-意见目标>三元组构成. 其中，“like”是意见措辞（Expression), “I”是意见持有者（Holder），“eating apples”是意见目标（Target）.

\begin{figure*}[hb!]
    %\vspace{-0.5em}
    \centering
    \includegraphics[angle=0,width=0.6\textwidth]{img/example-orl-crop.pdf}
    \caption{ORL的意见角色. }
    \label{fig:example_orl}
    %\vspace{-1.5em}
\end{figure*}

为了缓解ORL任务训练数据规模小的缺点，之前的工作利用与之高度相似的SRL语料资源来增强ORL模型的识别能力.
Kim和Hovy（2006）\upcite{kim2006extracting}、Ruppenhofer等（2008）\upcite{ruppenhofer2008finding}把意见措辞作为SRL的谓词，通过训练好的SRL模型获得语义角色，并将语义角色映射到意见角色上.
Marasovi{\'c}和Frank（2017）\upcite{marasovic2017srl4orl}以SRL为辅助任务，采用不同的多任务学习框架，来学习ORL与SRL任务之间的共性. %，并区分任务间的差异性.
Zhang等（2019）\upcite{zhang2019enhancing}从训练好的SRL模型中提取神经特征作为SRL感知的词表示，然后将其作为ORL的模型的额外输入，从而解决了SRL带来的错误传递问题.

本文第四章首次全面探索依存句法信息在ORL任务中的应用. 我们通过不同的方法表示依存句法信息，并将其融入到基于神经网络的ORL模型中. 基于该研究内容，我们在ACL-2020会议上投稿了一篇学术论文（已录用）.

%\subsection{意见角色标注}
%意见角色标注（ORL）是一种细粒度的意见分析任务，旨在识别出意见的实体. 如图\ref{fig:example_orl}所示，意见实体由<意见措辞-意见持有者-意见目标>三元组构成. 其中，“like”是意见措辞（Expression), “I”是意见持有者（Holder），“eating apples”是意见目标（Target）.
%
%\begin{figure*}[hb!]
%	%\vspace{-0.5em}
%	\centering
%	\includegraphics[angle=0,width=0.7\textwidth]{img/example-orl-crop.pdf}
%	\caption{ORL的角色组成. }
%	\label{fig:example_orl}
%	%\vspace{-1.5em}
%\end{figure*}
%


%之前的研究关注于识别意见实体的部分或全部角色.
%Breck等（2007）\upcite{breck2007identifying}、Yang和Cardie（2014）\upcite{yang2014joint}只关注于识别意见措辞角色；
%而Kim和Hovy（2006）\upcite{kim2006extracting}、Johansson和Moschitti（2013）\upcite{johansson2013relational}则使用级联的方式，
%首先预测意见措辞，然后为每个意见措辞找到意见持有者和意见目标这两个角色；
%联合模型则同时识别所有意见角色，并且预测意见角色属于哪一个意见措辞\upcite{choi2006joint,yang-cardie-2013-joint,katiyar2016investigating}.
%本文遵循Marasovi{\'c}和Frank（2017）\upcite{marasovic2017srl4orl}、Zhang等（2019）\upcite{zhang2019enhancing}的ORL工作，在给定意见措辞的条件下，预测意见持有者和意见目标这两个角色.
%
%为了缓解ORL数据的稀缺性，之前的工作利用语义角色标注任务（Semantic Role Labeling, SRL）的语料资源来提升ORL模型的识别能力.
%因为SRL是一个与ORL高度相似的任务，并且有大量的训练数据.
%Kim和Hovy（2006）\upcite{kim2006extracting}、Ruppenhofer等（2008）\upcite{ruppenhofer2008finding}把意见措辞作为语义角色标注任务的谓词，通过训练好的语义角色标注模型获得语义角色，并将语义角色映射到意见角色上.
%Marasovi{\'c}和Frank（2017）\upcite{marasovic2017srl4orl}以SRL为辅助任务，讨论了不同的多任务学习框架，来学习ORL与SRL任务之间的共性，并区分任务间的差异性.
%Zhang等（2019）\upcite{zhang2019enhancing}从预先训练好的SRL模型中提取神经特征作为SRL感知的词表示，然后将其作为ORL的模型的额外输入，从而解决了SRL带来的错误传递问题.
%
%于此同时，Yang和Cardie（2013）\upcite{yang-cardie-2013-joint}通过模式将依存树融合到基于传统特征的CRF（Conditional Random Field）模型中. 然而，几乎没有工作研究句法对基于神经网络的ORL模型的作用. 据我们所知，本文是首次全面尝试研究如何将句法信息融入到基于神经网络的ORL模型中.

%等采用Kipf和Welling\upcite{kipf2016semi}提出GCN（Graph Convolutional Networks）

%利用谓词与其他词之间的相对位置来表示依存关系路径，
%而Roth和Lapata（2016）\upcite{roth2016neural}利用LSTM（Long Short-Term Memory）模型来获得依存路径的向量表示.
%Tai等（2015）\upcite{tai2015improved}、Kipf和Welling\upcite{kipf2016semi}分别提出使用TreeLSTM（Tree-structured Long Short-term Memory）和GCN（Graph Convolutional Networks）来编码依存句法树信息.
%而Xia等（2019）\upcite{xia2019mtl}、Zhang等（2019）\upcite{zhang2019syntaxMT}提出使用基于句法感知的预训练词向量来表示句法信息，并分别应用到SRL任务和机器翻译任务中.

%\subsection{意见角色标注}

\section{研究内容及章节安排}
本文主要研究了依存句法分析领域的两个备受关注的方向.
一是多源异构树库的利用问题. 我们首次提出并采用有监督的树库转化任务来利用异构树库，设计并实现了两种高性能的树库转化模型.
之后，我们从如何深度、高效地编码源端句法树以及如何合理利用转化后树库两个角度考虑，设计并实现了多种有效的解决方法，进一步提升了目标端句法模型的性能.
一是依存句法的应用场景和方法的探索.
我们首次在基于神经网络的意见角色标注任务中引入句法信息，显著增强了ORL模型的识别能力.
此外，引入了具有强大表征能力的BERT后，句法信息仍然能大幅提升ORL模型的性能.
具体地，本文共包含五章，各章组织内容如下：

第一章：绪论. 本章首先阐明了基于依存句法的树库转化及应用的背景和意义；然后详细介绍了依存句法分析、多源异构树库的融合、依存句法应用的相关工作；最后列出了本文的章节组织和内容.

第二章：基于模式嵌入和SP-TreeLSTM的树库转化.
本章针对前人提出的间接树库融合方法和无监督树库转化方法的缺点，提出有监督的树库转化方法. 我们选取了哈工大HIT依存树库（源端）和苏大CODT依存树库（目标端）作为案例并展开研究. 首先，我们采用CODT规范对HIT树库中的部分句子进行再次标注，形成了11k左右的双树对齐数据（一个句子同时具有两种不同的标注结构）；然后，我们采用浅层和深度编码源端句法树的方法，设计了两种性能相当的树库转化模型，即基于模式嵌入的树库转化方法和基于SP-TreeLSTM的树库转化方法. 最后，我们利用转化模型将源端树库转化为符合目标端规范的树库，并与人工标注的树库合并，训练一个目标端句法模型，显著提升了目标端CODT句法模型的性能.

第三章：基于Full-TreeLSTM的树库转化与树库融合.
采用树库转化方式利用多源异构树库包含两个关键环节：树库转化和转化后树库的利用（树库融合）. 本章从这两个环节出发，从理论和实验两方面分析了第\ref{sec:super_tc}章方法的不足之处，并提出相应的优化方法. 针对第\ref{sec:super_tc}章树库转化方法不稳定、低效的缺点，我们提出了兼具深度和高效的基于Full-TreeLSTM的树库转化方法. 树库融合方面，为了减弱转化后树库包含的噪音问题，增大人工标注数据对最终句法模型的影响，我们提出语料加权以及合并后微调这两种方法. 两份双树对齐数据上的实验结果表明，我们提出的优化方法不仅提升了转化模型的训练/测试速度（性能也略有提升），而且合理地使用了转化后树库，进一步提升了目标端句法模型的性能.

第四章：句法增强的意见角色标注.
本章在意见角色标注任务上探索了依存句法的应用. 首先，我们从基于图的BiaffineParser依存句法分析器的计算机制出发，从其不同模块中获取并比较了三种不同形式的句法信息，即句法感知隐状态、句法森林和句法树.
然后，我们采用了多种不同的方法来表示这三种形式的句法信息. 最后，我们分别采用级联方式和多任务学习框架将句法信息融入到ORL模型中. 在标准数据集MPQA上的五折交叉验证实验表明了句法信息和多任务学习框架的有效性，并达到了最新的SOTA性能. 此外，细节的实验分析表明句法信息与与强大的上下文的词表示（BERT）具有互补的贡献.


第五章： 总结与展望. 最后简要的总结了本文的主要工作内容，以及对未来工作的展望.

%\begin{enumerate}
%	\item 有监督的树库转方法研究
%
%	首次提出有监督的树库转化任务，设计并实现了两个高性能的树库转化模型.
%
%	为了能高质量地将源端树库zhua作为一种多源异构数据融合方法，树库转化可以直接有效的利用异构树库中的语言学知识，以提高目标树库上的句法分析性能.
%	我们第一次提出了有监督的树库转化任务. 首先，我们人工标注了超过10,000个句子的双树对齐数据.
%	然后，基于目前性能最好的句法分析模型，我们提出了两种简单有效的树库转化方法，即基于模式嵌入方法和基于SP-TreeLSTM方法；
%	实验结果表明1）两种树库转化方法达到了可比较的性能;2）在多树库融合中，树库转化方法优于广泛使用的多任务学习框架，最终的句法模型性能显著提高.
%
%	\item 基于Full-Tree TreeLSTM的树库转化与树库融合方法研究
%
%	在有监督的树库转化方法基础上，为了高效、深度编码源端句法树信息，首次尝试并提出基于Full-Tree LSTM的树库转化方法.
%	进而，为了减弱转化后树库中包含的噪音问题，尝试并提出语料加权和合并后微调两种树库融合方法.
%	两个双树对齐语料上的实验表明：1）和前人方法相比，所提出的Full-Tree LSTM树库转化方法更加有效；
%	2）语料加权以及合并后微调两种方法都可以更有效的融合转化后树库；
%
%
%	\item 句法增强的意见角色标注模型
%
%	意见角色标注(ORL)是一种细粒度的意见分析任务，旨在回答“谁对什么表达了什么样的情绪”. 由于人工标注数据的稀缺性，ORL对于数据驱动的方法仍然具有挑战性. 我们尝试通过比较和融合具有不同表现形式的句法信息来增强基于神经网络的ORL模型.
%	我们还提出了使用基于图卷积网络层对处于不同处理级别的句法信息进行编码.
%	为了弥补句法模型的错误解析，减少错误的传播，我们引入了新颖的多任务学习框架来同时训练句法模型和ORL模型.
%	我们在基准MPQA语料库上验证了我们的方法，实验结果表明，对于ORL来说，句法信息是非常有价值的，我们的MTL模型显著有效，比我们不用句法的基线ORL模型提高了15.8\%的F1分数. 此外，我们验证了句法信息的贡献并不完全与强大的上下文的词表示(BERT)重叠，并且我们的最好模型比当前的最好性能F值上提升了9.29\%.
%\end{enumerate}

