\chapter{绪论}
\label{cha:intro}

\section{研究背景和意义}

自然语言处理（Natural Language Processing, NLP）是目前人工智能方兴未艾的领域之一.
一个完整的自然语言处理句子分析流程主要分为三个部分：1）词法分析；2）句法分析；3）语义分析.
其中词法分析包含了词性标注（Part-of-Speech Tagging）、命名实体识别（Named Entity Recognition, NER）以及消歧（Disambiguation）等子任务，中文由于词语之间没有天然边界，还额外需要进行中文分词.
句法分析的目的是以句法树的形式，刻画句子词语的修饰关系，主要包含了依存句法（Dependency Parsing）和成分句法（Constituency Parsing）分析这两种句法分析范式.
语义分析的主要目的是为了理解句子内涵的语义，包含语义角色标注（Semantic Role Labeling, SRL）、语义依存分析（Semantic Dependency Parsing, SDP）和抽象语义表示（Abstract Meaning Representation, AMR）等子任务.
句子分析作为分析句子结构，理解句子结构的必不可少的步骤，对下游任务，如机器翻译（Machine Translation）、问答（Question \& Answering, QA），有极大的帮助作用.

上述的三个流程通常以管道的方式进行.
而句法分析作为连接词法和语义分析的中间步骤，具备十分重要的研究价值.
目前存在着多种句法分析文法，例如组合范畴文法（Combinatorial Cat-egorial Grammars, CCGs），成分文法（Constituency Grammars）或者我们更为熟知的上下文无关文法（Contex Free Grammars, CFGs），以及依存文法（Dependency Grammars）.
其中依存文法对应的依存句法解析，和成分文法对应的成分句法解析是目前最常见的句法分析范式，也是本文主要的研究对象.
依存句法分析的目的是识别句子中词语次的修饰的关系.
成分句法则是将句子刻画为多个层次关系的短语，组成树的结构.

得益于基准数据集宾州书库（Penn Treebank, PTB）的发布，以及深度学习技术的长足进步，目前句法分析方法领域得到了迅速的发展.
不同于传统句法分析方法十分依赖于离散特征的人工设计，由于深度神经网络的强大上下文编码能力，句法分析器的方法愈来愈有简单化的趋势.
其中依存句法分析器Biaffine Parser\citep{dozat-etal-2017-biaffine}正是符合这样的潮流：利用诸如双向LSTM或Transformer\citep{vaswani-2017-attention}等强大编码器得到上下文表示，然后采取一个简单的头选择训练目标训练每个词找到正确的头.
类似的，\citet{gaddy-etal-2018-whats}采用的成分句法分析器采用了一个二分类训练目标，判断每个位置是否能够组成一个区块.
由于其准确率高，速度快的特点，基于局部训练目标的分析器是目前最为流行的句法分析器.

然而，已有的句法分析器Biaffine Parser也存在一些固有的缺陷.
由于训练时没有显式的树结构约束，在解码时分析器仍然需要通过解码算法（例如Eisner算法、MST算法或CKY算法）来得到一棵合法的树，这就造成了训练和预测的不匹配.
由于分析器训练目标较强的独立性假设，尽管能够得到合法的树输出，但是我们没法得到相应的树概率，而在数据建模的时代，概率分布估计一直是一个核心问题\citep{le-zuidema-2014-inside}.

因此，在本文章节~\ref{cha:dep-crf}和章节~\ref{cha:con-crf}，分别在依存句法分析任务和成分句法分析任务上，我们考虑将以前流行的结构化学习目标引入到现有的神经网络模型中，我们尝试引入树形条件随机场（TreeCRF）在训练时最大化树的概率.
考虑到在前人的工作中，引入高阶特征一直对于句法分析性能的提升\citet{mcdonald-pereira-2006-online,chen-manning-2014-fast}很关键，因此我们还尝试在两种句法分析范式中尝试了采用兄弟等二阶子树特征.
在我们之前，已经有研究者尝试在神经网络模型中增加高阶建模和结构化学习\citep{zhang-etal-2019-empirical,falenska-kuhn-2019-non}.
然而，引入更多的高阶特征后结构化学习算法受限于高复杂度问题，限制了该类算法的广泛应用.
我们主要从两个方面尝试缓解这类算法的高复杂度问题.
第一，得益于GPU的并行计算能力，我们为结构化学习的精确推断算法，诸如（二阶）Inside算法等设计了精巧的批次化方法，使得推断过程能够在GPU上能够利用到并行计算大大加速.
第二，有益于集成了自动求导机制的深度学习库的出现，我们无需和前人一样需要在CPU上进行完整的Inside-Outside过程得到梯度，而是由结合自动求导的反向传播机制自动完成.

在机器学习社区中，研究者针在推断算法高复杂度，或者不可精确推断的问题时通常的做法是采用近似算法来进行近似推断.
因此在章节~\ref{cha:vi}中，我们还尝试了利用机器学习中的一个近似推断算法，平均场变分推断（Mean Field Variational Inference, \textsc{Mfvi}），近似得到后验概率，我们在依存句法和成分句法分析两种范式中分别设计了两种不同的因子图以及变分推断迭代算法，显著提升了句法分析方法的解析速度.

\section{相关工作}
\label{sec:relworks}

\subsection{高阶依存句法分析}

批次化技术已经在线性链条件随机场（linear-chain CRF）中广泛应用，但是在树形结构中这个要复杂得多.
\citet{eisner-2016-inside}在成分句法分析上提出了一个关于Outside算法和反向传播机制等价性的理论证明，并同样讨论了其他类似于依存文法的范式.
我们在章节~\ref{cha:dep-crf}的工作中借鉴了他们的理论性工作.
作为一个经验型分析，我们相信我们的尝试能够让树形条件随机场在现实系统中应用起来.

和\citet{gaddy-etal-2018-whats}在成分句法上的工作类似，\citet{falenska-kuhn-2019-non}提出了一个在依存句法上的很好的分析性工作.
通过将\citet{kiperwasser-goldberg-2016-simple}的一阶基于图的方法扩展到二阶，他们尝试探究有多少结构化上下文被双向LSTM编码器捕捉.
他们拼接3层LSTM的在$i,k,j$位置的输出向量来给邻接兄弟子树打分，并采用了Max Margin训练损失和二阶Eisner解码算法\citep{mcdonald-pereira-2006-online}.
基于他们相对负面的结果和分析，他们认为高阶建模是多余的，因为双向LSTM已经可以有效的隐式捕捉足够的结构化上下文.
他们同样在RNNs和句法的关系上进行了详细的调研.
在我们的工作中，我们使用了一个更强的基线模型，并且发现了相比于他们的工作更显著的UAS/LAS提升.
特别地，我们提出了深入的分析，显示显式建模高阶信息可以帮助句法模型，因此与双向LSTM编码器是互补的.


\citet{ji-etal-2019-graph}引入了高阶结构信息，并用在Biaffine Parser\citep{dozat-etal-2017-biaffine}上用图神经网络来隐式建模.
他们在MLP层和Biaffine层之间增加了三层的图注意力网络（graph attention network，GAT）\citep{velickovic-etal-2018-graph}作为组块.
第一层GAT使用MLP层的输出$\mathbf{r}_i^{h}$和$\mathbf{r}_i^{m}$作为输入，通过集聚邻居节点产生新的表示$\mathbf{r}_i^{h1}$和$\mathbf{r}_i^{m1}$.
类似的，第二层GAT在$\mathbf{r}_i^{h1}$和$\mathbf{r}_i^{m1}$上操作，产生$\mathbf{r}_i^{h2}$和$\mathbf{r}_i^{m2}$.
通过这种方式，一个节点渐渐的收集到多跳的高阶信息作为单条弧打分的全局证据.
训练时他们遵循了原始的头选择训练损失.
与此相对的，我们的工作采用的全局的TreeCRF损失并显式地将高阶分值引入到了Biaffine Parser.

\citet{zhang-etal-2019-empirical}探索了在一阶Biaffine Parser上结构化学习的作用.
他们在多语言数据上比较了局部头选择损失、全局Max Margin损失和TreeCRF损失的性能.
他们表明全局的TreeCRF损失总体上要稍微好于Max Margin损失，并且对大多数语言而言，结构化学习带来了虽然不多但是显著的LAS提升.
他们同样表明结构化学习，特别是TreeCRF，极大的提升了句子级匹配的准确率，这与我们的观察相仿.
此外，他们通过Cython programming显式在CPU上计算了Inside算法和Outside算法.
与此对应的，这里我们在Biaffine Parser上提出一个高效的二阶TreeCRF扩展，并且进行了更加深入的分析，证明了结构化学习和高阶建模的效果.

\subsection{成分句法分析}

由于效率低下，以前很少有关于条件随机场的成分句法分析的工作.
\citet{finkel-etal-2008-efficient}提出了第一个基于非神经网络的引入丰富特征的条件随机场成分句法分析器.
\citet{durrett-klein-2015-neural}拓展了\citet{finkel-etal-2008-efficient}的工作，使用一个带非线性激活的前馈神经网络来打分产生式.
这两个工作都在CPU上显式进行了Inside-Outside计算，并且有严重的效率问题.

我们的成分句法分析器基于高性能的基于双向LSTM编码器\citep{stern-etal-2017-minimal}的现代神经网络分析器，在基于图的模型上利用了minus feature\citep{cross-huang-2016-span}进行区块的打分.
最近有很多工作都参考了\citet{stern-etal-2017-minimal}.
\citet{gaddy-etal-2018-whats}尝试分析什么样的，以及有多少上下文被双向LSTM隐式编码.
\citet{kitaev-klein-2018-constituency}将2层双向LSTM替换为自注意力层，并通过分离上下文和位置注意力，发现了可观的提升.
与此对应的，我们的工作表明通过合理的设置双向LSTM，例如Dropout策略，\citet{stern-etal-2017-minimal}的分析器可以超过\citet{kitaev-klein-2018-constituency}的结果.
请注意\citet{kitaev-klein-2018-constituency}的工作中使用了很大的词向量.

对于序列标注任务而言，批次化很直接，并且已经被很好的解决，比如NCRF++的实现\footnote{\url{https://github.com/jiesutd/NCRFpp}}.
但是，还很少有工作关注树形结构.
在依存句法的工作里，我们首次提出批次化树形结构的Inside和Viterbi（Eisner）计算，以便于在依存句法分析中利用GPU加速\citep{zhang-etal-2020-efficient}.
现在，我们在成分句法分析中做了类似的扩展，采用了不同的Inside和Viterbi（CKY）算法.

Torch-Struct\footnote{\url{https://github.com/harvardnlp/pytorch-struct}}\citep{rush-2020-torch}是我们的方法之外一个同期独立完成的工作，同样为成分句法分析实现了批次化的CKY算法.
然而，Torch-Struct旨在实现通用目的的结构化预测算法的基本实现.
与此相反，我们着力于复杂的解析器模型，并力求达到成分句法分析在准确率和效率上的最佳性能.

与此同时，近期有许多工作没有显式考虑结构化约束或者CKY解码，极大简化了成分句法分析任务.
\citet{gomez-rodriguez-vilares-2018-constituent}通过对每个词设计复杂的标签编码树信息，尝试采用序列标注方法解决成分句法分析任务.
\citet{vilares-etal-2019-better}通过一系列的增强技术，例如多任务学习和策略提督，进一步增强的序列标注方法.
\citet{shen-etal-2018-straight}提出对于正确树中的每个邻居词对预测一个数值距离，并应用自底向上的贪婪解码找到一棵最优的树.
然而，所有上面的工作仍然在解析性能上极大的落后于主流的方法.

\subsection{近似推断方法}
有许多文献已经证明，引入高阶特征，即考虑多个变量之间的交互，能够帮助模型准确率的提升
\citep{mcdonald-pereira-2006-online,carreras-2007-experiments,koo-collins-2010-efficient,ma-zhao-2012-fourth}.
然而这种高阶特征通常导致难以设计一个精确推断的算法来对模型进行学习，甚至导致学习问题成为NP-hard问题\citep{mcdonald-pereira-2006-online}.
因此我们不得不考虑近似方法.
由于不可精确推断问题的广泛存在，一直以来近似推断算法在NLP社区都有很多都应用.

\citet{martins-etal-2009-concise}将依存句法分析问题近似为整数线性规划（Integer Linear Programming, \textsc{Ilp}）问题.
与传统算法相比，\citet{martins-etal-2009-concise}利用了大量的局部和全局丰富特征，例如兄弟、祖父特征，价键特征，作为线性约束，用\textsc{Ilp}来求解，保证输出是一棵合法的依存树.
\citet{koo-etal-2010-dual}将非投影依存句法分析形式化为对偶分解（Dual Decomposition, \textsc{Dd}）问题.
将一个难以求解的问题分解为若干个可求解的子问题，例如可以将输出树的问题分解为单条弧、连续兄弟、双头等\citep{martins-etal-2011-dual}.

另一个和我们的在本文采用的平均场变分推断高度相关的方法是循环置信传播（Loopy Belief Propagation, \textsc{Lbp}）.
循环置信传播定义了一个包含多种局部和全局特征的由变量和因子构成的因子图，接着算法迭代式地让因子（变量）收集邻居变量（因子）的信息，最终收敛后得到后验概率.
\cite{smith-eisner-2008-dependency,gormley-etal-2015-approximation}在依存句法分析上引入了循环置信传播.
除了上面提到的兄弟（\textsc{Sib}）、祖父（\textsc{Grand}）因子，他们还引入了丰富的全局因子，例如\textsc{Exactly1}约束每个词有且仅有一个头，\textsc{Tree}要求所有词必须组成一棵树等等.
\citet{naradowsky-etal-2012-grammarless}将循环置信传播引入到了成分句法分析，也采用了类似的高阶因子和\textsc{Tree}因子在成分句法的变体.
与精确算法相比，循环置信传播可以以可接受的复杂度引入大量全局因子，并为模型带来了可观的性能收益.

还有一些工作同时使用了上述的近似算法.
\citet{auli-lopez-2011-comparison}在组合范畴文法中同时采用了循环置信传播\citep{smith-eisner-2008-dependency}以及对偶分解\citet{koo-etal-2010-dual}，取得了最佳性能，并对两种方法做了经验性比较.
\citet{martins-etal-2010-turbo}提出了在非投影依存句法分析中同时利用了循环置信传播\citep{smith-eisner-2008-dependency}以及整数线性规划\citep{martins-etal-2009-concise}方法，表明两种方法采用了一致的因子图，并在局部近似的目标函数的理念上是相一致的.

\section{章节和内容安排}

本文共分为五个章节，各章节具体安排如下:

第一章 绪论. 本章介绍本文中每个工作的任务背景和意义，并阐述一下与工作相关的有关文献的进展和内容，以及与我们方法的对比.

第二章 基于树形条件随机场的高阶依存句法分析.
本章在当前最佳的依存句法分析器的基础上，提出了一个二阶树形条件随机场的拓展，进一步提升了句法分析的性能.
为了解决效率问题，我们还提出了批次化技术在GPU上对训练和解码算法进行加速.

第三章 基于树形条件随机场的快速精准成分句法分析.
本章将树形条件随机场的应用到了神经成分句法分析器当中.
我们应用了批次化技术解决了树形条件随机场效率低下的问题.
我们提出了一个简单的两阶段解析方法，来进一步提升分析器的效率.
我们还为解析器引入了新的打分架构，以及有效的Dropout策略，使得解析器达到了新的最佳水平，并且解析速度很快.

第四章 基于变分推断的高效句法分析方法.
本章在前面章节提出的高阶句法解析器的基础上，提出利用平均场变分推断方法来近似推断后验概率.
在引入高阶算法的同时，变分推断避免了精确推断算法的高复杂度问题，并达到了和精确推断方法接近或相当的结果.

第五章 总结与展望. 本章总结本文的主要内容，并展望后续可能的研究方向.