\chapter{基于变分推断的TreeCRF加速}
\label{cha:vi}

本章节在章节~\ref{cha:dep-crf}和~\ref{cha:con-crf}提出的高阶句法分析器的基础上，尝试利用基于平均场变分推断来近似得到树的后验概率，并与精确推断的CRF相比较.
我们在依存句法和成分句法的中英文常见的5个基准数据集上做了实验，并对比了加入BERT之后的效果.
结果表明，我们采用的变分推断近似方法不仅从准确率上可以与章节~\ref{cha:dep-crf}和~\ref{cha:con-crf}里提出的二阶基于图的解析器相匹敌，并且训练和测试的速度要远远快于二阶CRF解析器.

\section{引言}\label{sec:vi-intro}
近年来句法分析领域有了长足的进步.
研究者针对句法分析任务提出了一系列的方法 \citep{dozat-etal-2017-biaffine,gomez-rodriguez-vilares-2018-constituent,ji-etal-2019-graph,zhang-etal-2020-fast,wei-etal-2020-span}，将英语基准数据集宾州树库（Penn Treebank, PTB）的准确率刷新到了很高的水平.
本章中我们主要关注于基于图的句法分析方法.

基于图的方法选择将一棵句法树分解为多个部分，然后分别打分，组成树的分值，并训练模型使得能够找到分值最大的句法树.
其中最简单的一阶方法让树中的每个变量都互相独立.
在深度学习时代之前，一阶方法通常需要结合结构化学习 \citep{mcdonald-etal-2005-online,koo-etal-2007-structured,taskar-etal-2004-max}.
得益于深度神经网络的强大上下文建模能力，近期的一些工作通常基于更简单的训练方法，其中\citet{dozat-etal-2017-biaffine}（Biaffine Parser）提出一个简单的基于头选择目标的解析器，训练时最大化每条弧的头的概率.
\citet{gaddy-etal-2018-whats}则提出一个基于二分类目标的成分句法分析方法，训练模型判断每个可能位置是否组成组块.
由于其高效率，并且取得了不逊色于结构化学习方法的结果 \citep{zhang-etal-2019-empirical,falenska-kuhn-2019-non}，这些方法是目前最为流行的句法分析方法.

与此对应的，研究者通常也尝试更多结构化约束引入神经网络模型中.
一方面利用TreeCRF、Max Margin \citep{ma-hovy-2017-neural,falenska-kuhn-2019-non,stern-etal-2017-minimal}等结构化学习算法来全局最大化句法树的概率.
另一方面，则是尝试利用高阶特征，建模兄弟、祖父等子树结构 \citep{mcdonald-pereira-2006-online,koo-collins-2010-efficient}.
我们在章节~\ref{cha:dep-crf}和章节~\ref{cha:con-crf}的工作证明，在神经网络模型中，引入结构化学习给句法分析任务会给带来了一定的提升.
在依存句法分析中我们还进一步尝试了二阶兄弟特征的高阶句法分析，是依存分析的结果超越了一系列前人的工作，达到了最佳水平.

尽管在神经网络上被证明有效，高阶结构化学习方法的一些固有缺陷限制了其应用.
由于需要精确推断得到树概率，这需要$O(n^3)$的时间复杂度，因此大大慢于一阶方法.
此外，如果进一步考虑其他高阶特征，例如共同父亲等等，这可能会导致动态规划结构难以设计，或者算法复杂度难以忍受.

考虑到这种限制，因此，在本章中我们参考前人的一些工作 \citep{smith-eisner-2008-dependency,wang-etal-2019-second,wang-tu-2020-second}，在利用高阶特征的同时，尝试用平均场变分推断（Mean Field Variational Inference, MFVI）方法来近似获得后验概率.
在本章中，我们在依存句法分析和成分句法分析这两个句法分析任务上尝试了应用基于变分推断的近似学习算法.
针对依存解析和成分解析两种任务的特性，我们分别在依存句法的变分推断中参考\cite{wang-tu-2020-second}引入了头选择的结构约束，而在成分句法中则采用了二分类的学习目标.
我们发现近似算法在获取了和精确推断接近的准确率的同时，解析速度能够大大提高.

% 近似方法 \citep{smith-eisner-2008-dependency,gormley-etal-2015-approximation}，并使用$AD^3$ \citep{martins-etal-2011-dual,martins-etal-2013-turning}来进行解码.
总体而言，我们在本章节的工作如下：
\begin{itemize}
  \item 我们提出在句法分析任务上应用引入二阶特征的变分推断方法近似得到后验概率来进行结构化学习，并和前述章节的精确推断方法做了比较.
  \item 针对依存句法分析和成分句法分析任务的特性，我们分别探索了基于头选择的变分推断方法和基于二分类的变分推断方法用到两种句法分析任务上.
  \item 我们在中文和英文数据上做了实验，在加入BERT之后，我们的变分推断方法在一些数据集上达到了新的最佳结果，
        并且，在依存和成分句法分析上分别可以达到到1,126句和905句每秒的解析速度.
\end{itemize}

\section{基于MFVI的二阶模型}\label{sec:vi-2o-approach}

本章在章节~\ref{cha:dep-crf}以及章节~\ref{cha:con-crf}的基础上，分别设计了一个基于变分推断的二阶依存句法和成分句法模型.
本章保持了前面章节提出的二阶模型架构，区别是用MFVI近似算法代替了精确推断的TreeCRF方法，从而大大降低了复杂度.

\subsection{模型定义}\label{sec:vi-model-definition}

我们保持了和精确推断模型一致的两阶段解析策略.

\noindent\textbf{第一阶段}：给定输入句子$\boldsymbol{x}$，目标是找到一棵最优（概率最高）的无标签树$\boldsymbol{y}=\arg\max_{\boldsymbol{y}^{\prime}}p(\boldsymbol{x},\boldsymbol{y}^{\prime})$.
在精确推断方法中，为了得到树概率，我们需要通过$O(n^3)$复杂度的Inside算法计算配分项$Z(\boldsymbol{x})\equiv\sum_{\boldsymbol{y}^{\prime}\in\mathcal{Y}}\mathrm{s}(\boldsymbol{x},\boldsymbol{y}^{\prime})$，来对分值$\mathrm{s}(\boldsymbol{x},\boldsymbol{y})$全局归一化，因此十分低效.

在本节我们提出利用高效的基于平均场理论的变分推断法（Mean Field Variational Inference, MFVI）来近似得到后验概率的方法.
MFVI假设句法树$\boldsymbol{y}$每个位置的变量相互独立，因此可以在线性时间内通过不断迭代得到后验概率的近似分布$Q(\cdot)$.
关于MFVI的通用更新公式以及相关推导见附录~\ref{appendix:mfvi-derivation}.
根据任务目标的不同，我们为依存句法和成分句法分别设计了两种不同的因子图，以及相应的更新方法.

\noindent\textbf{第二阶段}：我们的目标是为预测的无标签树$\boldsymbol{x}$的每个部分预测标签，得到标签序列$\boldsymbol{l}$.
依存树上$\boldsymbol{l}$被分解为每条弧的标签，成分树上$\boldsymbol{l}$由每个组块的标签组成.
我们采取和前述一样的方法，以贪婪解码的方式，为无标签依存句法树的每条弧$i\rightarrow j$找到一个分值最大的标签，成分句法树的每个组块$(i,j)$找到一个最优的标签.

\subsection{二阶子树打分}

具体而言，给定一个句子$\boldsymbol{x}$，我们的模型将对应的词向量输入到3层双向LSTM来计算每个位置的上下文表示，其中第$i$个词的输出为$\mathbf{h}_i$.
接着，我们利用$\mathbf{h}_i$对句法树的每个子结构进行打分.
对于依存句法，我们利用公式~\ref{eq:biaffine}得到每条弧$i\rightarrow j$的分值$\mathrm{s}(i\rightarrow j)$，对于成分句法，我们用公式~\ref{eq:con-biaffine}得到每个组块$(i, j)$的分值$\mathrm{s}(i, j)$.

我们在变分推断方法中还引入了二阶子树特征.
其中，依存句法中，我们采用了和章节~\ref{cha:dep-crf}一样的兄弟子树结构.
我们利用公式~\ref{eq:triaffine}得到兄弟子树$i\rightarrow \{k,j\}$的分值$\mathrm{s}(i\rightarrow\{k,j\})$，其中$k$是$j$的兄弟并且两者的父亲均为$i$.
成分句法中，参考章节~\ref{cha:con-crf}，我们利用公式~\ref{eq:con-triaffine}得到$\mathrm{s}(i,k,j)$，代表由$(i,k)$和$(i,j)$两个组块构成的子树$(i,k,j)$的分值，$k$和$j$分别是两个组块的右边界位置，并且左边界均为$i$.

对于标签的分值，我们利用Biaffine结构，分别得到依存树每条弧上的每个标签的分值$\mathrm{s}(i\rightarrow j,l)$，以及成分句法树每个组块上的每个标签的分值$\mathrm{s}((i,j),l)$.

\subsection{依存句法的MFVI方法}\label{sec:dep-vi}

\begin{figure}[tb!]
  \centering
  \begin{subfigure}[b]{0.8\textwidth}
    \centering
    \includegraphics[scale=1]{figures/dep-factors.pdf}
    \caption{依存句法模型的因子图}
    \label{fig:dep-factors}
  \end{subfigure}
  \begin{subfigure}[b]{0.8\textwidth}
    \centering
    \includegraphics[scale=1]{figures/con-factors.pdf}
    \caption{成分句法模型的因子图}
    \label{fig:con-factors}
  \end{subfigure}
  \caption{一个例句和其依存/成分句法模型对应的因子图，我们在例句上方标出了对应的正确无标签依存句法树和成分句法树，其中成分句法树进行了左二叉化.
    灰色虚线圆圈代表被屏蔽的变量.
    图中标出了所有的一阶（红色）因子，为了简洁起见，对于二阶因子，依存句法图中我们只标出了涉及弧$3\rightarrow 4$的兄弟（绿色）因子，成分句法图中只标出了和组块$(3, 4)$连接的二阶（蓝色）因子.}
  \label{fig:vi-factors}
\end{figure}

目前广泛使用的一阶局部模型（对应于章节~\ref{cha:dep-crf}的\textsc{Loc}方法，这里我们称为\textsc{Loc}$_{dep}$）在训练的时候采用了头选择（head selection）的训练损失函数，要求句子中除了根结点之外的每个词有且仅有一个头.
和\citet{wang-tu-2020-second}一样，我们选择在依存句法对应变分推断方法中引入头选择约束，相应的因子图设计见图~\ref{fig:dep-factors}.
对于每个位置$j$，我们定义变量取值$y_j\in \{0,1,\cdots,i\neq j,\cdots,n\}$，代表词$w_j$的可能的头索引.

具体地，对于每个变量$y_j$，一阶的势函数定义为
\begin{equation}
  \label{eq:dep-1o-potential}
  \psi_j(y_j)=\exp(s(y_j\rightarrow j))
\end{equation}
$s(y_j\rightarrow j)$是弧$y_j\rightarrow j$对应的分值，由公式~\ref{eq:biaffine}计算得到.

对于包含两个变量$y_{j}$和$y_{k}$的二阶因子，对应的势函数定义为
\begin{equation}
  \label{eq:2o-dep-potential}
  \psi_{j,k}(y_j,y_k)=\left\{
  \begin{array}{rcl}
    \exp \mathrm{s}(y_j\rightarrow \{k,j\}) &  & {y_j=y_k}   \\
    1                                       &  & {otherwise}
  \end{array}
  \right.
\end{equation}
$s(y_j\rightarrow \{k,j\})$是兄弟子树$y_j\rightarrow \{k,j\}$的分值，由公式~\ref{eq:triaffine}计算得到.
需要注意的是这里的兄弟$k$和二阶TreeCRF不同，并不需要一定和$j$邻近 \citep{smith-eisner-2008-dependency}.
我们要求子树$y_j\rightarrow \{k,j\}$中没有环存在，即有$k\neq {0,j,y_j}$.

MFVI迭代式地更新近似分布$Q(\cdot)$，来最小化其与真实分布$P(\cdot)$的KL散度.
依存句法模型的迭代更新公式如下 \citep{wang-tu-2020-second}：
\begin{equation}
  \label{eq:mfvi-dep}
  Q_{j}^{(t)}(i)\propto \exp\left(s(i\rightarrow j) +\sum_{k\neq i,j} Q_{k}^{(t-1)}(i)\cdot s(i\rightarrow {k,j}) \right)
\end{equation}
后验分布$Q_j^{(0)}(i)$初始化为一阶势函数的值$\psi_j(i)$.
每次迭代时，我们都会将$Q_j^{(t)}(\cdot)$的值在所有可能的头取值上进行归一化.
经过$T$次迭代后，我们得到最终的近似后验分布$Q_j^{(T)}(\cdot)$.

\subsection{成分句法的MFVI方法}\label{sec:con-vi}

我们的成分句法分析模型采用了\citet{gaddy-etal-2018-whats}的方法作为基线方法，称为\textsc{Loc}$_{con}$.
具体来说，模型对短语树所有可能的位置进行一个简单的二分类，预测该位置是否是一个区块.
\citet{dozat-manning-2018-simpler,wang-etal-2019-second}在语义依存分析中应用了这样的训练目标.
\citet{gormley-eisner-2015-structured,naradowsky-etal-2012-grammarless}在成分句法分析中应用了这种方法.
我们将其引入到了基于MFVI的成分句法分析中，并采用了一个一阶因子和一个二阶因子，对应的因子图见图~\ref{fig:con-factors}.

具体地，每个位置$ij$的可能的变量取值$y_{ij}\in \{0,1\}$.
其中单个变量$y_{ij}$的一阶的势函数定义为
\begin{equation}
  \label{eq:con-1o-potential}
  \psi_{ij}(y_{ij})=\left\{
  \begin{array}{rcl}
    \exp\left(\mathrm{s}(i,j)\right) &  & {y_{ij}=1}  \\
    1                                &  & {otherwise}
  \end{array}
  \right.
\end{equation}
$s(i,j)$是由公式~\ref{eq:con-biaffine}计算得到的区块$(i,j)$的分值，这里我们要求$i<j$.

给定两个变量$y_{ij}$和$y_{lk}$，我们使用章节~\ref{cha:con-crf}的二阶兄弟特征，因此二阶的势函数定义为
\begin{equation}
  \label{eq:2o-con-potential}
  \psi_{ij,lk}(y_{ij},y_{lk})=\left\{
  \begin{array}{rcl}
    \exp\left(\mathrm{s}(i,k,j)\right) &  & {i=l}       \\
    1                                  &  & {otherwise}
  \end{array}
  \right.
\end{equation}
$s(i,k,j)$可以视为$(i,j)$和$(i,k)$都作为组块时，组成的子树的分值，由公式~\ref{eq:con-triaffine}计算得到.
需要注意的是这里$k$的位置不受动态规划算法的约束，并不要求一定位于$(i,j)$之间，即有$i<k,k\neq j$.

成分句法模型的MFVI迭代更新公式如下：
\begin{equation}
  \label{eq:mfvi-con}
  \begin{array}{l}
    Q_{ij}^{(t)}(0)\propto 1 \\
    Q_{ij}^{(t)}(1)\propto \exp\left(s(i,j) +\sum_{k\neq i,j} Q_{ik}^{(t-1)}(1)\cdot s(i,k,j) \right)
  \end{array}
\end{equation}
后验概率$Q_{ij}^{(0)}(y_{ij})$初始化为一阶势函数$\psi_{ij}(y_{ij})$.
每次迭代时，我们将$Q_{ij}^{(t)}(\cdot)$在取值$\{0,1\}$上进行归一化.
经过$T$次迭代后，我们得到成分句法最终的近似后验分布$Q_{ij}^{(T)}(\cdot)$.

\subsection{训练损失函数}

我们的训练损失函数分为两部分，分别是无标签树的损失和对应标签的损失.
给定所有正确标签，我们的目标是最大化树上每个标签的概率，和前面的章节一致，我们采用了弧/区块级别的标准交叉熵损失函数.
给定输入句子$\boldsymbol{x}$和对应正确的无标签句法树$\boldsymbol{y}^{\ast}$，我们的目标是最大化句法树的概率$p(\boldsymbol{y}^{\ast}\mid\boldsymbol{x})$.
由于MFVI将真实分布在每个变量上分解得到近似概率分布$Q(\cdot)$，因此训练目标等价于最大化每个变量的后验概率.

其中，对于每个句子，依存句法分析树采用了弧分解策略，每个位置的变量取值为所有可能的头，因此相应无标签句法树的损失函数为
\begin{equation}
  \label{eq:dep-vi-arc-loss}
  L_{dep}^{tree}=-\sum_{j\neq 0}\log Q_j(y^{\ast}_j)
\end{equation}

成分句法的无标签句法树按照组块分解，每个组块可能的取值为$\{0,1\}$，因此目标函数为
\begin{equation}
  \label{eq:con-vi-bracket-loss}
  L_{con}^{tree}=-\sum_{i<j}\log Q_{ij}(y^{\ast}_{ij})
\end{equation}
参考\citet{dozat-manning-2018-simpler}，我们还新引入了一个参数$\lambda$用于平衡无标签句法树以及标签的损失，相应的成分句法的最终训练目标为
\begin{equation}
  \label{eq:con-vi-loss}
  L_{con}=\lambda L_{con}^{label}+(1-\lambda)L_{con}^{tree}
\end{equation}

\subsection{解码}
我们在MFVI模型中解码时直接应用了MBR解码的策略.
我们采用MFVI近似得到的后验概率$Q(\cdot)$代替边缘概率作为解码算法的输入.
依存句法中，我们将由公式~\ref{eq:mfvi-dep}得到的概率$Q_i(\cdot)$作为输入，并利用了Eisner算法来解码，加速时采用了算法~\ref{alg:eisner-2o}的批次化技术.
成分句法中，我们将由公式~\ref{eq:mfvi-con}得到的概率$Q_{ij}(\cdot)$作为输入，并利用了和章节~\ref{cha:con-crf}一样的类CKY算法来解码，采用了算法~\ref{alg:inside}的批次化技术来加速.

\section{实验}\label{sec:vi-exp}

\noindent\textbf{参数设置.}
我们保持两种句法分析模型的编码器和训练方法与前面的章节基本一致.
对于二阶模型，我们设置依存句法分析中使用的兄弟特征以及成分句法分析使用的二阶特征的MLP层输出维度为100.
我们设置变分推断的迭代次数统一为3次.
对于在成分句法分析中的用于平衡标签和无标签树的训练损失的参数$\lambda$，我们设置$\lambda$为0.1.

\noindent\textbf{模型定义.}
我们在每种句法分析上都比较了四种模型：\textsc{Loc}、\textsc{Crf}、\textsc{Crf2o}和\textsc{Mfvi}，这里有必要对于他们的记号做一下说明.
\textsc{Loc}表示采用了局部损失函数的模型，\textsc{Crf}和\textsc{Crf2o}指的是使用了一阶和二阶精确推断的TreeCRF模型，\textsc{Mfvi}表示采用平均场变分推断的模型.
相同记号的在各自任务的指导不同，例如依存模型中的\textsc{Loc}采用了头选择损失函数，而成分模型的\textsc{Loc}采用了二分类损失函数.
有时候我们会附加下标$dep$和$con$以示区分.

\input{tables/vi-dep-test.tex}

\subsection{主要结果}
表格~\ref{table:vi-dep-test}和表格~\ref{table:vi-con-test}分别给出了我们尝试的多种推断算法在依存句法和成分句法分析上的比较性实验结果.
和章节~\ref{cha:dep-crf}以及章节~\ref{cha:con-crf}一样，我们在所有模型中都使用了预训练词向量作为输入.
英文数据我们统一使用了100维的Glove词向量，中文数据则使用了word2vec在Giga数据上训练的100维词向.
我们还汇报了BERT的结果，对于英文数据，我们使用了24层1024维的\texttt{bert-large-cased}作为双向LSTM的输入，并在训练时固定参数.
中文数据我们则使用了12层768维的\texttt{bert-base-chinese}.

可以看到在依存句法的结果上（表格~\ref{table:vi-dep-test}），由于英文PTB的结果非常高，因此这四种推断方法的结果都十分接近，\textsc{Mfvi}方法超越了\textsc{Loc}，达到了最好的性能.
中文CoNLL09上，精确推断的\textsc{Crf2o}仍然是最好的，但是\textsc{Mfvi}超越了\textsc{Crf}达到了86.25的LAS.
在两种语言上，\textsc{Mfvi}都一致超越了没有全局结构约束的\textsc{Loc}方法.
这验证了在近似推断的\textsc{Mfvi}方法上应用二阶结构约束的有效性.

在成分句法分析上（表格~\ref{table:vi-con-test}），中英文三个数据的结果中，无论是一阶\textsc{Crf}还是二阶\textsc{Crf2o}，精确推断算法都仍然是表现最好的.
然而\textsc{Mfvi}都达到了和一阶\textsc{Crf}十分相近的性能.
并且，\textsc{Mfvi}一致超越了基于二分类学习的\textsc{Loc}模型，在PTB、CTB51和CTB7上分别提升了0.11、0.52和0.35.

当使用BERT之后，上述的几种推断算法在结果上没有显著差异.
依存句法上，PTB中\textsc{Mfvi}表现最好，LAS为95.37，达到或超越了当前的最佳性能 \citep{zhou-zhao-2019-head,wang-tu-2020-second}，CoNLL09上人仍然\textsc{Crf2o}最佳，但是\textsc{Mfvi}的差距十分微弱.
成分句法中在PTB和CTB51这两个数据上，\textsc{Mfvi}的表现都是最好的，分别达到95.71和92.56的$\mathrm{F}_1$值，高于当前最佳的模型\citep{kitaev-etal-2019-multilingual}.
在CTB7上表现最好的仍是\textsc{Crf2o}，$\mathrm{F}_1$值为91.62，而\textsc{Mfvi}的差距不到0.1.
然而，基于\textsc{Mfvi}方法的近似推断在解析效率上有很大的优越性（见章节~\ref{sub@sec:vi-speed}的复杂度分析）.

\input{tables/vi-con-test.tex}

\subsection{样例分析}

\begin{figure}[tb!]
  \centering
  \begin{subfigure}[b]{0.9\textwidth}
    \centering
    \includegraphics[scale=0.75]{figures/dep-probs.pdf}
    \caption{依存句法树每个位置对应的分值和后验概率}
    \label{fig:dep-probs}
  \end{subfigure}
  \begin{subfigure}[b]{0.9\textwidth}
    \centering
    \includegraphics[scale=0.75]{figures/con-probs.pdf}
    \caption{成分句法树每个位置对应的分值和后验概率}
    \label{fig:con-probs}
  \end{subfigure}
  \caption{一个例句对应的依存句法模型和成分句法模型的输出. 左边的图是每个位置的分值（log potential），为了方便显示，我们首先对分值进行了归一化.
    右边的图是变分推断得到的后验概率. 灰色虚线圆圈代表被屏蔽的不合法位置.}

  \label{fig:vi-probs}
\end{figure}

图~\ref{fig:vi-probs}给出了一个例句分别在依存和成分句法模型上的分值（log potentials）与经过MFVI迭代近似得到的后验概率的热力图对比.
图中颜色的深浅反应了分值/概率的大小.

左边的图反应的是模型直接输出的分值之间的对比，这对应于一阶模型的输出.
可以看到模型的输出的分值数值相对来说都比较均匀.
由于依存句法模型基于的是头选择的训练目标，可以看到图中的每一列都有一个极值存在，比如词$She_i$对应的列中，颜色最深（分值最高）的位置为词$enjoys_2$，这对应了句法树中的弧$enjoys_2\rightarrow She_1$.
成分句法的热力图中，由于采用了二分类的目标，因此颜色较深的位置表示更可能成为一个区块.
然而由于没有类似于依存句法的头约束，直接通过对每个位置$\arg\max$得到的所有区块可能无法组成一棵合法的句法树，因此我们仍需要CKY算法来获得全局最优的合法句法树.

当我们利用MFVI算法，得到的近似后验概率的热力图如右边图所示.
可以看到，在进一步考虑了二阶特征的分值并聚合到每个变量位置之后，后验概率的置信度远远高于分值.
对于依存句法而言，每个词都有一个最可能的头，并且概率几乎为1，对应于热力图中每列仅有一个红色结点.
对于成分句法而言，如果对图中每个位置的后验概率取$\arg\max$得到可能的区块，每个区块的概率同样近似为1，并且对于图中的例句而言，这种贪婪解码得到的句法树可以直接组成一棵合法的短语结构树，例如，红色结点$(She_1,tennis_4)$和$(playing_3,tennis_4)$对应了例句正确的无标签树左二叉化之后的两个区块.
这显示了MFVI在引入二阶结构打分之后，对于模型结构预测更强大的约束作用.


\subsection{速度和时间复杂度比较}
\label{sub@sec:vi-speed}
\input{tables/vi-speed.tex}
表~\ref{table:vi-speed}给出了\textsc{Mfvi}和前面章节提及的精确推断模型的速度和复杂度的比较.
为了统一比较，所有的设置下都应用了MBR解码，并且每个任务都用了相同的解码算法.
可以看到，无论是依存句法还是成分句法模型，他们相应的\textsc{Mfvi}在CPU上每一次迭代都需要$O(n^3)$的复杂度，这和精确推断的2阶Inside算法的复杂度相当.
而当利用GPU进行并行计算时，\textsc{Mfvi}每个位置的变量仅需要一次遍历来收集其他位置的信息\footnote{尽管现代的CUDA技术可以通过二叉树等数据结构让并行化的归约操作，例如$\mathrm{sum}$、$\mathrm{min}$和$\mathrm{max}$等，缩减到$O(\log n)$复杂度的时间 \citep{wang-etal-2020-ain}，但是这里我们统一假设归约操作的复杂度为$O(n)$.}，因此GPU上的算法复杂度为$O(n)$，大大快于精确推断所需要的$O(n^2)$复杂度.

从表中可以看到，利用\textsc{Mfvi}的依存句法在PTB的Test数据的解析速度为1126句每秒，是精确推断的二阶\textsc{Crf2o}（431）的近三倍快，同样也大大快于一阶\textsc{Crf}的653句每秒.
成分句法的\textsc{Mfvi}模型的解析速度大约为905句每秒，同样显著快于一阶\textsc{Crf}的743句每秒和二阶\textsc{Crf2o}的598句每秒.

\section{本章小结}

在本章我们将包含二阶特征的\textsc{Mfvi}方法方法引入到了依存句法和成分句法分析中.
根据两种句法分析的特点，在依存句法分析上，我们参考 \citep{wang-tu-2020-second}实现了一个包含头选择约束的\textsc{Mfvi}方法，在成分句法分析上，我们引入了一个基于二分类学习目标的\textsc{Mfvi}方法.
在这两个任务的中英文个五个数据集上的结果表明，\textsc{Mfvi}方法均超越了基于局部学习的\textsc{Loc}方法，并达到和精确推断的二阶\textsc{Crf2o}方法可比较的性能.
此外\textsc{Mfvi}方法相比于精确推断的解析速度大大加快.
在加入BERT之后，基于\textsc{Mfvi}方法的近似推断模型在所有的数据集上都达到或者接近了当前最佳的模型.