% !Mode:: "TeX:UTF-8"

% 中英文摘要
\begin{cabstract}

    依存句法分析旨在将词语序列转化为树状结构，以有向边为基本单元刻画词语间的修饰关系，是自然语言处理（NLP）的关键问题之一.
    目前学术界主要通过改进模型和算法来提升依存句法分析的准确率. 随着深度学习的发展，基于神经网络的句法模型取得了长足的进步. 本文从数据角度出发，采用树库转化方式利用多源异构树库来提升句法模型的性能，为NLP下游任务提供更可靠的句法结构.
    进而，本文利用目前最先进的依存句法分析模型产生不同形式的句法信息，并采用多种不同的方法将句法信息融入到意见角色标注任务中. 具体而言，本文的主要研究内容如下：

    %是计算机理解自然语言的重要表达形式之一.

    %依存句法分析是自然语言处理（NLP）中的关键问题之一. 目前研究界主要通过改进模型和算法来提升句法模型的改进模型和算法，d'l 提升准备, 从数据角度出发 可靠.
    %进而，应用.
    %
    %如何提升依存句法准确率以及如何应用依存句法信息，一直是研究界关注的方向.
    %一方面，高精度的依存句法分析器可以为NLP下游任务提供更可靠的句法结构.
    %另一方面，合理地利用依存句法信息能为NLP下游任务带来更大的增益.
    %本文的研究内容涵盖了这两方面：1）我们采用树库转化方式利用多源异构树库中存在的共同的语言学知识，以提升目标端句法模型性能；2）我们利用不同形式的依存句法信息，并采用多种不同的方法将句法信息融入到意见角色标注任务中. 本文的主要工作如下：

    \begin{enumerate}
        \item 基于模式嵌入和SP-TreeLSTM的树库转化

              作为一种多源异构数据融合方法，树库转化可以直接有效的利用异构树库中的语言学知识，以提高目标端句法分析的准确率.
              %多源异构树库中存在大量共同的语言学知识，
              %不同于间接的树库融合方法和无监督的树库转化方法，
              我们首次提出有监督的树库转化任务来利用多源异构树库. %，以弥补目标端树库规模小的缺点.
              首先，我们人工标注了约11K个句子的双树对齐数据；
              然后，基于目前性能最好的句法分析模型，我们设计并实现了两种简单有效的树库转化方法，
              即基于模式嵌入的方法和基于SP-TreeLSTM的方法；
              最后，我们利用转化模型将源端句法树转化为目标端句法树，扩大了目标端树库规模，提升了目标端句法模型性能.
              实验结果表明，在异构树库的利用上，有监督的树库转化方式优于广泛使用的多任务学习框架，显著提升了目标端句法模型的性能.

        \item 基于Full-TreeLSTM的树库转化与树库融合

              %There are two main challenges for treebank exploitation via treebank conversion. One is how to convert dsrc to dtgt with high quality (treebank conversion), and the other is how to eﬀectively exploit the converted treebank for higher parsing accuracy of target side (treebank exploitation）
              采用树库转化方式利用异构树库有两个挑战，一是如何将源端树高质量地转化为目标端树（树库转化），一是如何有效地利用转化后树库提升目标端句法模型性能（树库融合）.
              在第\ref{sec:super_tc}章的基础上，我们尝试改进树库转化和树库融合方法.
              树库转化方面，为了高效、深度编码源端句法树信息，我们首次尝试并提出基于Full-TreeLSTM的树库转化方法.
              树库融合方面，为了减弱转化后树库中包含的噪音问题，我们尝试并提出语料加权和合并后微调两种树库融合方法.
              在两份双树对齐语料上的实验表明：1）和第\ref{sec:super_tc}章的方法相比，基于Full-TreeLSTM的树库转化方法预测速度更快，且性能更优；
              2）语料加权以及合并后微调两种方法缓解了转化后树库中存在的噪音问题，进一步提升了目标端句法模型的性能.

        \item 句法增强的意见角色标注

              %我们在意见角色标注任务（ORL）上探索了依存句法的应用方法.
              意见角色标注是一种细粒度的意见分析任务，旨在回答“谁对什么表达了怎样的情感”，在现实生活中有很广泛的应用. %比如社交媒体监控，票房预测和电子商务应用.
              由于标注数据规模小，ORL对于数据驱动的方法仍然具有挑战性.
              %我们尝试通过比较和融合具有不同表现形式的句法信息来增强基于神经网络的ORL模型.
              %我们还提出了使用基于图卷积网络层对处于不同处理级别的句法信息进行编码.
              我们通过引入依存句法来缓解ORL训练数据的稀缺性问题.
              首先，我们从当前最先进的句法分析模型中抽取了三种形式的依存句法信息；
              %然后，我们尝试并比较了不同的编码方法表示三种形式的句法信息.
              %最后，我们分别采用级联方法和多任务学习方法将句法信息融入到ORL模型中.
              然后，我们尝试并比较了不同的编码方法表示这三种形式的句法信息，并采用级联的方式将这些句法信息融入到ORL模型中；
              最后，为了缓解级联方式带来的错误传递问题，我们引入了一种新颖的多任务学习框架来同时训练句法模型和ORL模型.
              %首先，基于现有的依存句法分析模型，我们指出并分析了三种形式的依存句法信息；然后通过多种不同的方法将句法信息融入到基于BiLSTM-CRF的ORL模型中.
              %在MPQA标准数据集上的五折交叉验证实验表明，句法增强的方法能够显著提高ORL模型性能，F1值上提升了3.50%. 在引入表征能力更强的BERT特征后，句法信息仍然可以进一步增强ORL模型的识别能力. 我们最好的模型大幅提升了ORL模型的性能，在F1值上超过基线模型8.40%，超过了目前最好的模型3.45%.
              %为了弥补句法模型带来的错误传递问题，我们引入了新颖的多任务学习框架来同时训练句法模型和ORL模型.
              我们在标准数据集MPQA上构建了多组对比实验. 实验结果表明：1）句法信息对于ORL任务非常有价值，句法信息的引入大幅提升了ORL模型的识别能力；
              2）相较于级联的句法应用方式，基于软参共享的多任务学习框架有效缓解了句法的错误传递问题，进一步提升了ORL模型性能.
              此外，我们验证了句法信息的贡献并不完全与强大的上下文的词表示(BERT)重叠，并且我们最好的模型比当前最好的性能在F1值上提升了4.34\%.

    \end{enumerate}

    综上，我们深入研究了多源异构树库的转化和融合问题，进而将依存句法的分析结果应用于ORL任务中. 我们希望这些初步的进展能够为依存句法分析以及自然语言处理领域中其它任务的发展提供帮助.

    \vskip 21bp
        {\heiti\zihao{-4} 关键词：}
    依存句法分析，
    双树对齐数据，
    有监督树库转化，
    意见角色标注，
    多任务学习

    \begin{flushright}
        作~~~~~~~~者：张宇

        指导老师：李正华

    \end{flushright}


\end{cabstract}


