\chapter{总结与展望}

\section{总结}
得益于深度学习技术的迅速发展，以及神经网络模型的强大建模能力，近年来句法分析领域得到了长足的进步.
\citet{dozat-etal-2017-biaffine}提出的Biaffine Parser作为当前最流行的句法分析器，代表了这样一个发展趋势：采用一个强大的编码器，结合一个简单的训练目标.
本文基于Biaffine Parser，在依存句法和成分句法分析两个任务的基础上，探索了进一步提升句法分析模型的途径.

考虑到在前人工作中，结构化学习方法和高阶建模是提升准确率的流行途径，因此在本文中，首先引入了将基于树形条件随机场的结构化学习方法，并与局部学习方法做了深入详细的对比.
其次本文将二阶特征引入到结构化学习建模，训练时最大化带二阶分值的树概率，发现进一步带来了准确率的提升.

考虑到效率问题一直是困扰结构化学习流行的因素之一，本文从两个方面改进.
第一，本文首次尝试在高复杂度的精确推断的结构化学习算法中引入批次化技术，使得算法能够受益于GPU并行计算的威力，计算速度显著提升.
第二，本文引入了机器学习社区中的变分推断作为近似推断算法，在保留高阶特征的同时，大大降低了得到后验概率的复杂度.

总的来说，本文的主要内容如下:
\begin{enumerate}
    \item 基于树形条件随机场的高阶依存句法分析\\
          \indent 本文在以当前最佳的神经依存句法分析模型Biaffine Parser为基础，提出了一个二阶TreeCRF的扩展，对于二阶子树，本文提出了一个新颖的Triaffine结构来打分.
          我们提出对于$O(n^3)$复杂的Inside算法进行批次化，来利用当前GPU并行计算的能力，将算法复杂度降低到了$O(n^2)$，使得二阶模型的解析速度达到400句每秒，相比于传统在CPU上运算的方式有数十倍的提升，并且没有明显慢于一阶模型.
          我们在13个语言的27个数据集上进行了大量的分析和实验，发现二阶模型带来了显著的准确率提升，并且尤其在全局指标，以及局部标注数据上表现良好.
    \item 基于树形条件随机场的快速精准成分句法分析\\
          \indent 本文提出在已有神经网络模型的基础上应用树形条件随机场到成分句法分析中.
          为了解决高复杂度问题，我们将训练和解码算法进行了高度批次化，从而支持在GPU上的大规模张量并行计算，大大降低了算法复杂度，带来了显著的效率提升.
          我们用支持自动求导机制的反向传播传播算法代替了Inside-Outside算法的显式计算得到梯度，进一步提升了效率.
          此外，我们提出了简单的两阶段解析方法，比前人的一阶段解析更加高效，并且没有损害性能.
          为了提升解析效果，我们提出用双仿射打分机制代替前人的minus feature，并且发现对双向LSTM编码器进引入的一些诸如Dropout的参数修改可以极大提升解析的性能，达到不输于Transformer的效果.
          在中英文三个数据集上表明，我们提出的新解析器显著超越了前人的方法，并且可以解析超过1,000句每秒，在使用BERT之后，我们的模型达到了新的最佳结果.
    \item 基于变分推断的高效句法分析方法\\
          \indent 考虑到精确推断的结构化学习算法的高复杂度问题，本文中我们将包含二阶特征的平均场变分推断引入到了依存句法和成分句法分析中，使得GPU上算法复杂度从$O(n^2)$降低到了$O(n)$.
          根据两种句法分析任务的不同学习特性，我们采取了不同的变分推断更新策略，在依存句法分析上我们采取了一个包含头选择约束的更新方法，在成分句法分析上我们则引入了一个基于二分类学习目标的更新方法.
          我们在中英文共五个数据集上做了实验，结果表明我们的方法显著超越了采用局部学习目标的方法，并达到了和精确推断的二阶方法可比较的性能.
          在使用BERT之后，基于变分推断的方法在所有数据上都达到或接近了现有模型的最佳水平.
\end{enumerate}

\section{未来展望}
本文在依存句法和成分句法分析两个任务上分别尝试了基于树形随机场的融入高阶特征的结构化学习方法，让句法分析模型达到了新的最佳水平，本文还尝试了应用基于变分推断的近似推断算法，大大提升了句法分析速度.
未来，本文打算基于已有成果从一下几个方面继续探索:
\begin{enumerate}
    \item 本文的句法分析模型全部基于Biaffine Parser，采用了3层双向LSTM作为编码器.
          考虑到Transformer模型的迅速发展，以及BERT预训练语言模型的强大作用，我们将尝试利用Transformer替换已有编码器，探讨自注意力机制的效果，以及不同语言模型的利用方式，例如特征集成和微调对模型性能的影响.
    \item 本文中为了方面起见我们只采用了一种高阶特征，例如依存模型中我们只采用了邻接兄弟特征，成分模型中采用了相邻区块特征.
          在未来我们将尝试更多的特征设计，探讨他们对模型效果的影响.
    \item 在本文中我们只尝试了基于平均场变分推断的近似推断算法.
          然而仍然有其他知名的近似算法在NLP社区有广泛的应用，例如循环置信传播、对偶分解、整数线性规划等等.
          在未来我们打算尝试更多近似算法，并对他们做一些经验性比较.
\end{enumerate}