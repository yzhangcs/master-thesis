
\chapter{总结与展望}

\section{总结}
依存句法分析将句子的词语序列转化为刻画词语间修饰关系的树状结构，连接了词法分析和语义分析，是自然语言处理（NLP）中的关键环节.
由于其形式简单、易于标注、便于应用等优势，依存句法分析被广泛采纳和研究. 多家机构组织了以依存句法分析为核心的评测任务，发布了多种语言的依存句法树库，为依存句法分析的研究打下了基础. 近年来，随着神经网络的发展，依存句法分析的准确率不断提高. 尤其是Dozat和Manning（2017）\upcite{dozat2017deep}提出的基于图的BiaffineParser句法分析模型，在多个公开数据集上取得了最高的准确率.

然而作为一种数据驱动的模型，训练数据的规模一定程度上制约着依存句法分析的准确率. 考虑到树库构建所需的人工成本，构建一个极大规模的人工树库是不切实际的.
此外，虽然许多工作探索了依存句法分析的应用问题，但是“如何编码依存句法的树状结构信息”，“如何解决句法分析带来的错误传递”等问题仍然是具有挑战的.
本文从提升依存句法分析模型的准确率和探索依存句法的使用方式两方面展开研究. 一方面我们研究了通过树库转化方式来利用多源异构树库提升目标端句法模型性能，另一方面我们在意见角色标注任务上探索了依存句法分析在的应用方式. 总之，本文的主要工作如下：

\begin{enumerate}
      \item 基于模式嵌入和SP-TreeLSTM的树库转化

            本章通过构造一个约11K个句子的双树对齐数据，首次提出了有监督的树库转化的任务. 然后，我们基于目前性能最好的句法分析模型BiaffineParser设计了两种简单且有效的树库转化方法，即基于模式嵌入的转化方法和基于SP-TreeLSTM的转化方法. 实验结果表明：1）两种树库转化方法达到了可比较的转化性能； 2）源端树中的依存关系标签是非常有用的特征，能进一步提高了两种转化方法的性能；3）源端树库的引入有效地提升了目标端句法模型性能，且树库转化方法比多任务学习方法更好地利用了源端树库，取得了更高的句法分析准确率. 综上，有监督的树库转化方法是一种简单有效的利用异构树库改善目标端句法模型性能的方式.

            %针对前人提出的间接树库融合方法和无监督树库转化方法，我们提出有监督的树库转化方法来利用异构树库.
            %首先，我们通过局部标注的方式快速构建了一个包含约11K个句子的双树对齐树库，并随机切分Train/Dev/Test集.
            %然后，我们通过扩展前人的工作，设计并实现了基于模式嵌入的转化模型和基于SP-TreeLSTM的转化模型，分别从浅层、深层利用源端树指导目标端树的生成.
            %令人意外的是，浅层的基于模式嵌入方法却达到了和深层的SP-TreeLSTM方法几乎一样的转化性能. 而且二者的结合并没有进一步提升转化性能，反而因为信息冗余导致了性能的降低.
            %此外，特征消解实验表明了源端树的依存标签信息大幅提升了LAS指标.
            %最后，我们结合转化后的源端树库，训练了一个目标端句法模型. 其性能不仅超过了只利用人工标注的数据训练的句法模型，而且超过了多任务学习框架训练的目标端句法模型，
            %表明了有监督的树库转化方法是更有效的利用异构树库的方式.


            %作为一种多源异构数据融合方法，树库转化可以直接有效的利用异构树库中的语言学知识，以提高目标树库上的句法分析性能.
            %我们第一次提出了有监督的树库转化任务. 首先，我们人工标注了超过10,000个句子的双树对齐数据.
            %然后，基于目前性能最好的句法分析模型，我们提出了两种简单有效的树库转化方法，即基于模式嵌入方法和基于最短路径TreeLSTM方法；
            %实验结果表明1）两种树库转化方法达到了可比较的性能;2）在多树库融合中，树库转化方法优于广泛使用的多任务学习框架，最终的句法模型性能显著提高.

      \item 基于Full-TreeLSTM的树库转化与树库融合

            本章是第二章的改进和优化，为了增强实验结果和结论的可靠性，我们又构建了一个树库对齐数据.
            我们分析并指出了第二章树库转化方法和树库融合方法的不足之处：
            1）基于模式嵌入的树库转化方法的不稳定性；2）基于SP-TreeLSTM的树库转化方法的低效性；3）直接合并转化后数据和人工标注数据忽略了转化后数据中包含噪音问题.
            针对以上问题，
            我们通过改进SP-TreeLSTM方法，提出了基于Full-TreeLSTM的树库转化方法，高效、深度地利用了源端句法树，进一步提升了转化性能；
            提出的语料加权和合并后微调两种树库融合方法减弱了转化后树库中包含的噪音问题，合理地使用了转化后数据，进一步提升了目标端句法模型的性能.
            此外，我们在实验中还发现了TreeLSTM输出层的dropout对Full-TreeLSTM方法影响较大，合理的设置dropout大小可以获得更高的转化性能.

            %为了解决和基于最短路径TreeLSTM方法的低效性，我们提出了
            %进而，我们提出语料加权和合并后微调两种树库融合方法加大了人工标注数据为了减弱转化后树库中包含的噪音问题，尝试并提出.
            %两个双树对齐语料上的实验表明：1）和前人方法相比，所提出的Full-Tree LSTM树库转化方法更加有效；
            %2）语料加权以及合并后微调两种方法都可以更有效的融合转化后树库；


      \item 句法增强的意见角色标注

            %本节针对意见角色标注任务训练数据规模小的缺点，提出引入依存句法这一语言学知识来弥补其训练数据的稀缺性.
            %基于目前性能最好的依存句法分析器BiaffineParser，我们抽取了三种形式的句法信息，即显式的句法树，显式的句法森林，隐式的句法感知隐状态.
            %随后，我们采用多种基于级联方式的方法表示句法信息，并进一步扩展到基于多任务学习的框架上.
            %实验结果表明了句法信息能有效的提升ORL模型的识别能力，基于软参共享的多任务学习框架有效地缓解了句法的错误传递问题，进一步提升了ORL模型的性能.
            %我们还发现，显式和隐式句法信息的结合能给ORL模型带来更多的帮助.
            %此外， 我们在添加BERT特征的基础上以同样的方式引入句法信息，ORL模型的性能仍然有大幅的提升.
            %%细致的实验分析表明相对于BERT，句法信息更擅长解决长距离依赖问题.
            %最后我们的最好的模型（结合句法和BERT）在F1值上超过基线模型9.29\%，超过了目前最好的模型4.34\%.
            本章针对意见角色标注任务训练数据规模小的缺点，提出引入依存句法这一语言学知识来弥补其训练数据的稀缺性.
            首先，我们从BiaffineParser的计算机制出发，从不同模块中抽取出了三种不同形式的依存句法信息，即句法树，句法森林和句法感知隐状态. 然后，我们采用不同的编码方法表示这些句法信息，并分别采用级联的方法和多任务学习框架将这些句法信息融入到基于BiSLTM-CRF的ORL模型中，有效地缓解了训练数据的稀缺性，显著提升了ORL模型的性能. 此外，在BERT的基础上加入句法信息的实验表明了BERT特征并不能完全取代句法信息的作用，二者从不同方向帮助了ORL模型. 同时融合句法和BERT的最优实验在F1值上达到了68.08\%，超过了基线模型9.29\%，超过了目前的最好性能4.34\%.
            %我们采用多种不同的方法表示句法信息，并融入到基于双向LSTM-CRF的ORL模型中，
\end{enumerate}

%综上，本文在依存句法的树库转化和应用上做了一些尝试并取得了一些成果. 然而，我们做的研究还不够全面，尤其是在依存句法的应用方面. 我们期待提出更好的表示依存句法信息的方法，并进一步探索句法的应用场景.
%总之本文研究了树库转化，以及依存句法在基于神经网络的ORL模型上的应用，取得了一些初步的成果. 我们希望这些研究成果能够为依存句法分析以及自然语言处理领域中其它任务的发展提供帮助.

\section{未来展望}

本文在依存句法的树库转化和应用上做了一些尝试并取得了一些成果. 然而，我们做的研究还不够全面，未来我们期待从以下几个方向进行探索:
\begin{enumerate}
      \item 树库转化的关键是如何充分利用源端句法树的信息，我们目前只尝试了有限的编码句法树的方法：基于模式嵌入方法，基于TreeLSTM方法.
            实际上，我们也尝试了采用GCN来编码句法树，但是转化效果不理想.
            未来，我们期待尝试更为合理有效的编码源端句法树的方式.
      \item 目前我们的树库转化方法是一种级联的方式：树库转化模型采用了多任务学习模型训练好的编码层.
            未来，我们期待设计一个联合的树库转化模型，同时训练多任务学习模型和树库转化模型，进一步提升转化性能.
            此外，目前我们构建的两份转化数据，目标端规范相同，源端规范不同.
            我们期待尝试多树库同时转化，充分利用已有源端树的信息.
      \item 我们提出的依存句法应用框架目前只在意见角色标注这一任务上取得了成果.
            未来，我们期待将提出的框架应用到更多的NLP任务中.
            此外，虽然本文同时融合了句法信息和BERT特征，但是二者的融合非常独立，没能去除二者之间的冗余信息.
            如何更加合理的使用句法和BERT特征也是一个很有趣的研究方向.
\end{enumerate}
