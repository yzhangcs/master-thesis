\chapter{总结与展望}

\section{总结}
得益于深度学习技术的迅速发展，以及神经网络模型的强大建模能力，近年来句法分析领域得到了长足的进步.
\citet{dozat-etal-2017-biaffine}提出的Biaffine Parser采用了一个强大的编码器结合一个简单的训练目标，是当前最流行的句法分析器.
与此对应的，传统的基于TreeCRF的全局训练目标尽管具有概率建模的优势，然而推断的低效问题限制了TreeCRF的广泛流行.
本文提出将TreeCRF应用到神经句法分析器当中，并借鉴前人工作提出了一个二阶TreeCRF拓展来进一步提升句法分析的准确率.
为了解决效率问题，本文提出了多项加速技术：1）批次化Inside算法；2）反向传播机制代替Outside算法.
二阶TreeCRF进一步增加了推断算法的复杂度，因此本文还提出利用基于平均场变分推断的近似算法代替二阶TreeCRF.
变分法的模型性能超越了一阶方法，并且在速度上显著超越了二阶TreeCRF.

总的来说，本文的主要内容如下:
\begin{enumerate}
	\item 基于TreeCRF的高阶依存句法分析\\
	      \indent 本文以当前最佳的神经依存句法分析模型Biaffine Parser为基础，提出了一个二阶TreeCRF的扩展.
	      对于二阶子树，本文提出了一个新颖的Triaffine结构来打分.
	      为了解决TreeCRF低效的问题，我们提出对于$O(n^3)$复杂的Inside算法进行批次化，利用GPU并行计算的能力将算法复杂度降低到了$O(n^2)$.
	      此外我们将复杂的Outside过程用基于自动求导机制的高效反向传播.
	      我们的加速方法让二阶模型的解析速度达到400句每秒，相比于传统在CPU上运算的方式有数十倍的提升，并且没有明显慢于一阶模型.
	      我们在13个语言的27个数据集上进行了大量的分析和实验，发现二阶模型带来了显著的准确率提升，并且尤其在全局指标，以及局部标注数据上表现良好.
	\item 基于TreeCRF的高阶成分句法分析\\
	      \indent 本文提出在已有神经网络模型的基础上应用高阶TreeCRF到成分句法分析中.
	      为了解决高复杂度问题，我们采用了和依存句法中一致的加速策略，将训练和解码算法进行了高度批次化，并且用支持自动求导机制的反向传播传播算法代替了复杂的Outside算法，从而显著提升了解析速度.
	      此外，我们提出了简单的两阶段解析方法，比前人的一阶段解析更加高效，并且没有损害性能.
	      为了提升解析效果，我们参考依存句法分析对模型架构进行了修改.
	      我们提出用双仿射打分机制代替前人的minus feature方法，并且发现在双向LSTM编码器中引入的一些诸如Dropout的参数修改可以极大提升解析的性能，达到不输于Transformer的效果.
	      在中英文的三个基准数据集上的实验结果表明，我们提出的一阶和二阶成分句法模型显著超越了前人的方法.
	      速度方面，一阶和二阶模型分别可以解析1,092和598句每秒.
	      在使用BERT之后，我们的模型在大部分数据集上都达到了新的最佳结果.
	\item 基于变分推断的高效句法分析方法\\
	      \indent 考虑到精确推断的TreeCRF方法的高复杂度问题，本文中我们将包含二阶特征的平均场变分推断引入到了依存句法和成分句法分析中，使得算法在GPU上的复杂度从$O(n^2)$降低到了$O(n)$.
	      根据两种句法分析任务的不同学习特性，我们采取了不同的变分推断更新策略，在依存句法分析上我们采取了一个包含头选择约束的更新方法，在成分句法分析上我们则引入了一个基于二分类学习目标的更新方法.
	      我们在中英文共五个数据集上做了实验，结果表明我们的方法显著超越了采用局部学习目标的方法，并达到了和精确推断的二阶TreeCRF方法可比较的性能.
	      在使用BERT之后，变分推断方法在所有数据上都达到或接近了现有模型的最佳水平.
\end{enumerate}

\section{未来展望}
本文在依存句法和成分句法分析两个任务上分别尝试了基于树形随机场的融入高阶特征的结构化学习方法，让句法分析模型达到了新的最佳水平。
本文还尝试了应用基于变分推断的近似推断算法，显著降低了高阶TreeCRF的复杂度，大大提升了句法分析速度。
未来，本文打算基于已有成果从一下几个方面继续探索:
\begin{enumerate}
	\item 本文的句法分析模型主要采用了3层双向LSTM作为编码器.
	      考虑到Transformer的迅速发展，以及BERT预训练语言模型的强大作用，未来我们将尝试利用Transformer替换已有编码器，探讨自注意力机制的效果，以及语言模型不同的利用方式，例如特征集成和微调对模型性能的影响.
	\item 本文中为了方便起见我们只采用了一种高阶特征，例如依存模型中我们只采用了邻接兄弟特征，成分模型中采用了区块分割点特征.
	      在未来我们将尝试更多的特征设计，探讨他们对模型效果的影响.
	\item 在本文中我们只尝试了基于平均场变分推断的近似推断算法.
	      然而，仍然有其他的近似算法在NLP社区有广泛的应用，例如循环置信传播、对偶分解、整数线性规划等等.
	      在未来我们打算尝试更多近似算法，并对他们做一些经验性比较.
\end{enumerate}