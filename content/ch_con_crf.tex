\chapter{基于树形条件随机场的快速精准成分句法分析}
\label{cha:con-crf}

本章节提出了一个基于树状条件随机场的快速精准的神经成分句法分析器.
估计概率分布一直是自然语言处理领域的一个核心问题.
但是，在深度学习时代和前深度学习时代，不同于线性链条件随机场（linear chain CRF）在序列标注任务中的大量应用，由于Inside-Outside算法的高复杂度，还很少有工作将树形条件随机场应用到成分句法分析任务当中.
这里我们提出应用树状条件随机场到成分句法分析，核心的想法是批次化计算损失函数用到的Inside算法，使其能够支持在GPU上的大规模张量并行计算，与此同时结合基于高效自动求导机制的反向传播，避免了复杂的Outside算法的计算.
我们同样提出一个简单的两阶段解析方法，bracketing-then-labeling，来进一步提升分析器的效率.
为了提升解析的性能，受依存句法分析器的启发，我们引入了一个基于边界表示和仿射注意力的新打分架构，以及一个有效的Dropout策略.
在PTB、CTB5.1和CTB7上的实验表明我们的两阶段条件随机场分析器在使用和不使用BERT的两种设置上，达到了新的最佳性能，并且解析速度达到了1,000句每秒.

\section{引言}\label{sec:con-intro}

\input{figures/con-tree.tex}
给定一个句子，成分句法分析旨在构建一个层次化的树结构. 如图\ref{fig:con-tree-full-figure}，其中每个叶子结点是输入句子的每个词，而非终端结点作为区块（Constituents），如\texttt{$VP_{3,5}$}.

成分句法分析是自然语言处理领域一个基础但是富有挑战性的任务.
由于诸如宾州树库（Penn Treebank，PTB）、中文宾州树库（Penn Chinese Treebank，CTB）等大规模树库的标注，成分句法分析吸引了一大批研究者的关注.
同样的，句法分析输出的句法树也被证明对于大量的下游任务\cite{akoury-etal-2019-syntactically,wang-etal-2018-tree}都有用.

作为最有影响力的工作之一，\cite{collins-1997-three}概率上下文无关文法 (Probabilistic Context-Free Grammars，PCFGs）扩展到了词汇化文法（Lexicalized PCFGs）.
由此开始，成分句法分析方法一直是这样的生成式模型（generative models）占据主导地位，并且其中广泛使用的Berkeley Parser采用了带隐式非终端结点标注的非词汇化概率上下文无关文法（Unlexicalized PCFGs）\cite{matsuzaki-etal-2005-probabilistic,petrov-klein-2007-improved}.
而在判别式方法（discriminative models），存在着两种主要方向.
第一种采取了以动态规划解码为基础的基于图的方法，训练时使用局部max-entropy估计\cite{kaplan-etal-2004-speed}或者全局max-margin方法\cite{taskar-etal-2004-max}.
第二类则通过基于贪婪解码或者集束搜索（beam search）产生shift-reduce这样的转移序列来构建一棵树，这种方法被称为基于转移的方法\cite{sagae-lavie-2005-classifier,zhu-etal-2013-fast}.

最近，得益于深度神经网络在上下文表示方面令人印象深刻的发展，成分句法分析取得了显著的进展.
其中，\cite{cross-huang-2016-span}的基于转移的分析器，以及\cite{stern-etal-2017-minimal}的基于图的分析器是两个具有代表性的工作.
作为判别式模型，两个分析器有很多的共同点，他们都使用了1）多层双向LSTM作为编码器；2）从双向LSTM的输出得到的minus features作为区块的表示；3）利用MLP层来为区块打分；4）max-margin的训练损失函数.
后续的大多数工作\cite{gaddy-etal-2018-whats,kitaev-klein-2018-constituency}的主要设置都和这两个分析器一样， 并且都相比传统的非神经网络模型达到了更好的准确率，这特别是得益于由在大规模无标记文本上训练的语言模型输出的上下文词表示的使用\cite{peters-etal-2018-deep,devlin-etal-2019-bert}.

然而尽管有这些显著的进展，现有的成分句法分析的研究仍然受两个相互关联的缺点困扰.
首先，解析速度（训练速度同理）很慢，并且很难满足现实系统的需要.
其次，显式的树/子树概率建模的缺失一定程度上影响来分析器输出的利用.
一方面，估计概率分布一直是自然语言处理领域的核心问题\cite{le-zuidema-2014-inside}.
另一方面，与没有上下姐的树的分值相比，树的概率可以作为一种软特征，更好的被更高层级的任务所利用\cite{jin-etal-2020-relation}，且子树的边缘概率可以支持更加复杂的最小贝叶斯风险解码\cite{smith-smith-2007-probabilistic}.

事实上，\cite{finkel-etal-2008-efficient,durrett-klein-2015-neural}都通过建模树的条件概率$p(\boldsymbol{t}\mid\boldsymbol{x})$，提出了基于CRF\cite{lafferty-etal-2001-crf}的成分句法分析器.
但是，由于损失函数和梯度计算需要的Inside-Outside算法的高复杂度（尤其是Outside算法），这些模型都极端低效.
而在深度学习时代，由于以前所有的工作都直接在CPU上运行Inside-Outside算法，而让模型频繁在CPU和GPU切换所需要的时间代价是昂贵的，因此效率问题变得更加严重.

本章节通过极大拓展\cite{stern-etal-2017-minimal}的基于图的分析器，提出了一个CRF成分句法分析器.
主要的贡献在于我们为损失函数和梯度能够在GPU上能够直接计算，类似于章节\ref{cha:dep-crf}，我们提出了Inside算法的批次化方法.
与此同时，我们发现Outside算法的可以通过自动的反向传播机制被高效的完成，这验证了\cite{eisner-2016-inside}出色的理论性能做，使得outside过程与Inside一样高效.
类似的，我们也批次化了CKY（Cocke–Kasami–Younger）算法，以支持高效的解码.

总体而言，我们做了如下的贡献.
\begin{itemize}
    \item 为了直接建模树和子树的概率，我们首次提出了一个快速精准的CRF成分句法分析器.
          通过批次化技术支持Inside算法和CKY算法在GPU上的直接计算，长期以来一直困扰句法分析社区的效率问题在这里被很好的解决了.

    \item 我们提出了一个两阶段的解析方法bracketing-then-labeling：先产生无标记树骨干树（bracketing）再标注标签（labeling）的解析方法，这不仅更加高效，并且达到了比一阶段解析方法稍好的性能.

    \item 我们提出了基于区块表示的一个新的打分架构，以及基于仿射注意力机制的打分方法，比minus-feature方法表现的更好.
          我们同样表明，通过更好的模型及参数设置，比如Dropout，解析的性能可以被极大提升.

    \item 在中英文的三个基准数据集上的实验表明，我们提出的两阶段解析方法在使用BERT和不使用BERT\cite{devlin-etal-2019-bert})的两种设置下，性能上达到了新的最佳水准.
          解析速度方面，我们的分析器可以达到1,000句每秒的解析速度.
\end{itemize}

\begin{figure}[tb]
    \centering
    \includegraphics [scale=1.2]{figures/con-framework.pdf}
    \caption{模型架构.}
    \label{fig:con-framework}
\end{figure}

\section{两阶段CRF解析}\label{sec:2stage-parsing}

正式地，给定一个由$n$个词组成的句子$\boldsymbol{x}=w_0,\dots，w_{n-1}$，如图~\ref{fig:con-tree-original}所示，一棵成分句法树可以表示为$\boldsymbol{t}$，其中$(i,j,l) \in \boldsymbol{t}$是一个包含$w_{i}...w_{j}$的区块，对应的句法标签为$l \in \mathcal{L}$.
一棵树同样可以被分解为两个部分，即，$\boldsymbol{t}=(\boldsymbol{y}, \boldsymbol{l})$，其中$\boldsymbol{y}$是一棵无标签树（又称为bracketed tree），而$\boldsymbol{l}$是树中所有区块的标签按一定顺序产生的标签序列.
区块$(3,5，\texttt{VP})$可以等价表示为$\texttt{VP}_{3,5}$.

为了能够适应Inside算法和CKY算法，我们用NLTK工具包\footnote{\url{https://www.nltk.org}}将原始的树转换为了遵循乔姆斯基范式（Chomsky Normal Form，CNF）的二叉树形式，如图~\ref{fig:con-binaried-tree}所示.
特别地，连续的单链产生式被压缩为了一个标签，比如$\texttt{X}_{i,j} \rightarrow \texttt{Y}_{i,j}$会被压缩为一个$\texttt{X+Y}_{i,j}$.
根据前置实验，我们在二叉化原始树的时候，采用了左二叉.
在用CKY解码获得一棵最佳的树之后，CNF树会被恢复为原来的\textit{n}-ary形式.

\subsection{模型定义}\label{sub@sec:model-definition}

在这份工作中，我们对成分句法分析采用了一个两阶段的解析框架bracketing-then-labeling.
这与传统的一阶段解析方法\cite{stern-etal-2017-minimal,gaddy-etal-2018-whats}相比，不仅简化了模型架构，同样也提升了效率.

\noindent\textbf{第一阶段：bracketing.}
给定句子$\boldsymbol{x}$，第一阶段的目标是找到一个最优的无标签树$\boldsymbol{y}$.
一棵树的分值被分解为所有包含的区块的分值.
\begin{equation} \label{eq:tree-score}
    \mathrm{s}(\boldsymbol{x},\boldsymbol{y}) = \sum\limits_{(i,j)\in \boldsymbol{y}}\mathrm{s}(i,j)
\end{equation}
对于CRF，条件概率为
\begin{equation}\label{eq:tree-prob}
    \begin{split}
        & p(\boldsymbol{y}\mid\boldsymbol{x})  = \frac{\exp({\mathrm{s}(\boldsymbol{x},\boldsymbol{y}))}}{Z(\boldsymbol{x}) \equiv \sum\limits_{\boldsymbol{y'} \in \mathcal{Y}(\boldsymbol{x})} {\exp({\mathrm{s}(\boldsymbol{x},\boldsymbol{y'}))}}}
    \end{split}
\end{equation}
其中$Z(\boldsymbol{x})$被称为partition term，$\mathcal{Y}(\boldsymbol{x})$是输入句子$\boldsymbol{x}$对应的所有合法句法树的集合.

给定所有的区块分值$\mathrm{s}(i,j)$，我们可以用CKY算法来找到一棵最优的无标签句法树$\hat{\boldsymbol{y}}$.
\begin{equation} \label{eq:tree-argmax}
    \hat{\boldsymbol{y}} = \arg\max_{\boldsymbol{y}} \mathrm{s}(\boldsymbol{x}, \boldsymbol{y}) = \arg\max_{\boldsymbol{y}} p(\boldsymbol{y} \mid \boldsymbol{x})
\end{equation}

\noindent\textbf{第二阶段：labeling.}
给定一个句子$\boldsymbol{x}$和一棵树$\boldsymbol{y}$，第二阶段独立地给每个区块$(i,j) \in \boldsymbol{y}$预测一个标签.
\begin{equation} \label{eq:label-argmax}
    \hat{l} = \arg\max_{l \in \mathcal{L}} \mathrm{s}(i,j,l)
\end{equation}
请注意训练时我们使用正确的无标签树来进行损失函数的计算.
对于一个长度为$n$的句子，所有的CNF树都包含同样$2n-1$多个的区块.
因此，这一阶段的时间复杂度为$O(n|\mathcal{L}|)$.

\noindent\textbf{时间复杂度分析.}
CKY的时间复杂度为$O(n^3)$.
因此，我们两阶段解析方法的总时间复杂度为$O(n^3+n|\mathcal{L}|)$.
相对应的，对于一阶段解析的CKY而言，算法需要为所有$n^2$个区块决定最优的标签，因此需要$O(n^3+n^2|\mathcal{L}|)$，其中$|\mathcal{L}|$通常来说都很大（比如对于表~\ref{table:con-statistics}中的英语来说为138）.

\subsection{打分架构}

本章节引入了给区块和标签打分的模型架构，如图~\ref{fig:con-framework}，大部分设置都遵循\cite{stern-etal-2017-minimal}，除了两个重要的修改：1）针对分值计算的边界表示和仿射注意力；2）和\cite{Timothy-d17-biaffine}一样的更好的参数设置.

\noindent\textbf{模型输入.}
对于第$i$个词，其对应的输入向量$\mathbf{e}_i$是词向量和字级别表示的拼接：
\begin{equation} \label{eq:token-representation}
    \mathbf{e}_i = \mathbf{e}^{word}_i \oplus \mathrm{CharLSTM}(w_i)
\end{equation}
其中$\mathrm{CharLSTM}(w_i)$是将字序列输入到双向LSTM一样的输出向量\cite{lample-etal-2016-neural}.
以前的工作表明将词性表示用$\mathrm{CharLSTM}(w_i)$代替会带来稳定的提升\cite{kitaev-klein-2018-constituency}.
这同样简化了模型，因为不再需要额外预测词性.

\noindent\textbf{双向LSTM编码器.}
我们在输入向量上应用了三层双向LSTM以得到上下文表示.
我们分别用$\mathbf{f}_i$和$\mathbf{b}_i$来表示词$w_i$在顶层LSTM的前向和后向输出向量.

在这里，我们借用了大部分\cite{Timothy-d17-biaffine}的依存句法分析器的参数设置（参考章节\ref{sec:dep-exps}）.
我们发现其中的Dropout设置对于解析性能特别关键，这和原始的实现\cite{stern-etal-2017-minimal}有两方面的不同.

首先，对于每个词$w_i$，$\mathbf{e}^{word}_i$和$\mathrm{CharLSTM}(w_i)$都作为一个整体被dropout，要么保持版本，要么称为$\mathbf{0}$向量.
如果其中一个向量被设置为了$\mathbf{0}$，则另一个会被乘以2倍作为补偿.
其次，相同LSTM层在不同的时间步（词）共享相同的dropout掩码\cite{yarin-etal-2016-dropout}.

\noindent\textbf{边界表示.}
对于每个词$w_i$，我们参考\cite{stern-etal-2017-minimal}的做法来组成上下文词向量\footnote{我们的前置实验表明$\mathbf{f}_i \oplus \mathbf{b}_{i+1}$相比于$\mathbf{f}_i \oplus \mathbf{b}_i$有稳定的提升. 可能的原因是$\mathbf{f}_i$和$\mathbf{b}_i$都使用$\mathbf{e}_i$作为输入，因此提供的信息冗余.}
\begin{equation}
    \mathbf{h}_i = \mathbf{f}_i \oplus \mathbf{b}_{i+1}
\end{equation}
$\mathbf{h}_i$的维度为800.

不同于直接应用一个单一的MLP层到$\mathbf{h}_i$上，我们发现一个词在一棵给定树的所有的区块中都必须作为其左边界或右边界.
因此，我们应用来两个MLP层来做这样的区分，并且分别获取左边界和右边界的表示向量.
\begin{equation}
    \label{mlp-borlders}
    \mathbf{r}_i^{l}; \mathbf{r}_i^{r} =\mathrm{MLP}^{l} \left( \mathbf{h}_i \right); \mathrm{MLP}^{r} \left( \mathbf{h}_i \right)
\end{equation}
$\mathbf{r}_i^{l/r}$的维度$d$为500.
正如\cite{Timothy-d17-biaffine}所指出的，MLP层缩减了$\mathbf{h}_i$的维度，并且更重要的是只保留了句法相关的信息，因此能够减轻过拟合的风险.

\noindent\textbf{仿射打分.}
给定边界表示，对于候选区块$(i,j)$，我们在左边界词$w_i$的表示和右边界词$w_j$的表示上利用仿射注意力为该区块打分.
\begin{equation} \label{eq:biaffine}
    \mathrm{s}(i,j) =  \left[
        \begin{array}{c}
            \mathbf{r}_{i}^{l} \\
            1
        \end{array}
        \right]^\mathrm{T}
    \mathbf{W} \mathbf{r}_{j}^{r}
\end{equation}
其中$\mathbf{W} \in \mathbb{R}^{d \times d}$.

计算区块标签的分值$\mathrm{s}(i,j,l)$使用的是类似的方式.
需要在$\mathbf{h}_i$上应用额外两层MLP来获取相应的边界表示$\bar{\mathbf{r}}^{l/r}_i$（维度为$\bar{d}$）.
接着我们使用$|\mathcal{L}|$个Biaffine（$\mathbb{R}^{\bar{d} \times \bar{d}}$）来获取所有标签的分值.
由于$|\mathcal{L}|$非常大，我们对于$\bar{\mathbf{r}}^{l/r}_i$使用了稍小的维度$\bar{d}=100$（对于${\mathbf{r}}^{l/r}_i$则是500）以减轻内存和计算负担.

\noindent\textbf{前人打分方法.}
\cite{stern-etal-2017-minimal}对双向LSTM的输出使用了minus featured方法来得到区块的表示\cite{wang-chang-2016-graph,cross-huang-2016-span}，然后用MLP层得到区块的分值.
\begin{equation} \label{eq:minus-score}
    \mathrm{s}(i,j)=\mathrm{MLP}(\mathbf{h}_{i}-\mathbf{h}_{j})
\end{equation}
在实验部分我们表明我们的打分方法要明显更优越.

\subsection{训练损失函数}

对于一个训练的例子$(\boldsymbol{x},\boldsymbol{y},\boldsymbol{l})$，训练损失函数由两部分组成.
\begin{equation} \label{eq:final-loss}
    \mathit{L(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{l})} = \mathit{L}^{bracket}(\boldsymbol{x}, \boldsymbol{y}) + \mathit{L}^{label}(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{l})
\end{equation}
第一项是句子级的全局CRF损失，目的是最大化树的条件概率:
\begin{equation}\label{eq:bracket-loss}
    \begin{split}
        \mathit{L}^{bracket}(\boldsymbol{x},\boldsymbol{y})
        &= -\mathrm{s}(\boldsymbol{x}, \boldsymbol{y}) + \log Z(\boldsymbol{x})
    \end{split}
\end{equation}
其中$\log Z(\boldsymbol{x})$可以利用Inside算法在$O(n^3)$的时间复杂度内被计算.

第二项是在labeling阶段，区块级别的标准交叉熵损失函数.

\section{高效的训练和解码}
\label{sec:efficient-training-decoding}

\input{algorithms/con-inside.tex}

这里我们描述如何通过批次化Inside算法和CKY算法以支持在GPU上的直接计算，来进行高效的训练和解码.
我们同样表明对于成分句法分析，复杂的Outside算法可以被自动求导机制支持的反向传播完成.

\subsection{批次化的Inside算法}

对于公式~\ref{eq:bracket-loss}里的$\log Z(\boldsymbol{x})$和特征梯度，所有以前在CRF解析上的工作\cite{finkel-etal-2008-efficient,durrett-klein-2015-neural}都显式地在CPU上进行了Inside-Outside算法的计算.
不同于线性链CRF，树状结构的CRF看起来要复杂很多.

在这里，我们发现实现一个批次化的Inside算法是可行的，如算法~\ref{alg:inside}所示.
关键的想法是将一个批次的所有实例中宽度相同的区块分值打包到一个大的张量中.
这使得我们可以通过高效的大规模张量操作进行计算和合并.
由于对所有的$0 \le i$,$j<n$,$~r$,$0\le b<B$而言，在GPU上的计算都是并行的，因此算法仅需要$O(n)$步.
我们的代码会给出更多的技术细节.

\subsection{Outside算法的替代：反向传播}

传统上，Outside算法被认为对于子树边缘概率和特征梯度的计算是不可或缺的.
事实上，Outside算法通常至少要两倍慢于Inside算法.
而Outside算法的批次化也要复杂的多.
幸运的是，这个问题在深度学习时代被很好的解决了，基于自动求导机制支持的反向传播可以方便的获取梯度.
\cite{eisner-2016-inside}提出了理论上关于反向传播和Outside过程的等价性证明，和章节~\ref{cha:dep-crf}一样，我们给出的简化版本证明在附录~\ref{sec:outside-backprop}.
由于我们在前向过程使用来批次化的Inside算法，因此反向传播过程也是通过大规模张量并行计算的，可以视为同样高效.

值得注意的是通过用区块分值$\mathrm{s}(i,j)$对$\log Z(\boldsymbol{x})$求偏导（同样由自动的反向传播完成），我们可以自然的得到区块$(i,j)$的边缘概率，也就是梯度.
\begin{equation} \label{eq:partial-derivative}
    p((i, j)\mid\boldsymbol{x}) = \frac{\partial \log Z(\boldsymbol{x})}{\partial \mathrm{s}(i, j)}
\end{equation}
边缘概率在很多下游任务都可以作为一种很有用的软特征.
更多细节可以参考\cite{eisner-2016-inside}.

\subsection{解码}

正如上面提到的，解析过程中，我们应用CKY算法来获取一棵最佳的句法树，如公式~\ref{eq:tree-argmax}所示.
和Eisner算法一样，CKY算法和成分句法分析的Inside算法几乎一样，除了其中的sum-product被替换为了max product（参考算法~\ref{alg:inside}的行~\ref{line:sum-product}），因此可以被同样高效的批次化.
为了进行MBR解码，我们直接将区块分值$\mathrm{s}(i,j)$换成式~\ref{eq:tree-score}和式~\ref{eq:tree-argmax}的边缘概率$p((i,j)\mid\boldsymbol{x})$.
然而，我们发现这在成分句法上带来了很微弱的提升.

\input{tables/con-statistics.tex}

\section{实验}
\label{sec:con-experiments}
\noindent\textbf{数据.}
我们主要在三个中文和英文的数据集上进行实验.
前两个数据集，即PTB和CTB5.1，是句法分析社区中比较常用的两个数据集.
我们遵循了传统的train/dev/test数据的分割.
考虑到CTB5.1的dev/test都只有大约350句，为了得到更加稳定一致的结果，我们同样在更大的CTB7数据上进行了实验，相关的数据分割设置参考了官方手册建议.
表~\ref{table:con-statistics}列出了相关数据的统计信息.
可以看到CNF转换引入了很多新的区块标签，其中大部分（大约75\%）是源于连续单链的折叠过程.

\noindent\textbf{评价.}
正如前面提到的，在解析之后，我们将最佳的CNF树转化为了\textit{n}-ary树再进行评价.
这里有必要提及一些有用的细节.
由于解码算法没有相应的约束，预测的最佳CNF树可能包含很多不合法的产生式.
以图~\ref{fig:con-binaried-tree}为例，模型可能输出$\texttt{VP}_{3,5} \rightarrow \texttt{PP}^{\ast}_{3,3} ~ \texttt{NP}_{4,5}$，其中 $\texttt{VP}$和$\texttt{PP}^{\ast}$是不兼容的.
在\textit{n}-ary后处理过程中，我们直接忽略了``$\mathtt{\ast}$''符号之前具体的字符串$\texttt{PP}$.
有鉴于此，如果解码的时候增加一定的约束，结果有可能进一步提高，这里我们留待后续的工作.

我们使用了标准的区块级别的准确率、召回率和F值（P/R/F）作为评价指标，并使用\texttt{EVALB}工具\footnote{\url{https://nlp.cs.nyu.edu/evalb}}来评价.
特别地，一个诸如$\texttt{VP}_{3,5}$的预测区块如果出现在了正确树中，那就被认为是正确的.\footnote{
    由于一些研究者可能会实现他们自己的评价叫门，为了比较的公平，需要澄清一些细节：
    1）一些诸如\{-NONE-\}的空区块在预处理的时候被移除了.
    2）评价的时候作为根结点的区块（英语里是\{TOP，S1\}，中文里是空字符串) 被忽略了.
    3）包含一些例如\{:，``，''，.，?，!\}这些标点的区块也被忽略了. 请注意中文标点会作为正常的字符被评价.
    4）一些在同一集合中的标签，例如\{ADVP，PRT\}，被认为是等价的.}

\noindent\textbf{参数设置.}
我们直接采用\cite{Timothy-d17-biaffine}的依存句法分析器里面的大部分参数设置，没有进一步的改动.
唯一的区别是我们用了CharLSTM词表示，而非词性embedding.
CharLSTM里自向量、词向量以及CharLSTM输出向量的维度分别为50、100和100.
所有Dropout的比率为0.33.
一个批次数据的大小约为5,000个词.
训练过程持续至多不超过1,000次迭代，并且如果Dev数据上的最高结果连续100次不提示，那么训练就会提前停止.

\input{tables/con-dev.tex}

\subsection{Dev数据上的模型比较}

We conduct the model study on dev data from two aspects: 1) CRF vs. max-margin training loss; 2) two-stage vs. one-stage parsing.
The first three lines of
Table~\ref{table:con-dev} shows the results.
The three models use the same scoring architecture and parameters.
Following previous practice \cite{stern-etal-2017-minimal}，one-stage models use only scores of labeled constituents $\mathrm{s}(i,j,l)$.
%on the dev data，including the max-margin and our CRF approach.
% \footnote{We perform MBR decoding on all datasets for the CRF approach and the improvement brought by MBR is very slight. We ignore the exhibition of the results without MBR due to space limitation.}
% For fairness，we retain the biaffine scoring in the max-margin method.
In order to verify the effectiveness of the two-stage parsing，we also list the results of ``CRF (one-stage)''，which directly scores labeled constituents.
\begin{equation} \label{eq:tree-label-score}
    \mathrm{s}(\boldsymbol{x},\boldsymbol{y},\boldsymbol{l}) =
    %\sum_{(i,j)\in \boldsymbol{y}} {\mathrm{s}(i,j) +
    \sum_{(i,j,l) \in (\boldsymbol{y}, \boldsymbol{l})} \mathrm{s}(i,j,l)
\end{equation}
As discussed in the last paragraph of Section \ref{sub@sec:model-definition}，the inside and CKY算法s become a bit more complicated for the one-stage parser that two-stage.

From the first two rows，we can see that
under the one-stage parsing framework，the CRF loss leads to similar performance on English
but consistently outperforms the max-margin loss by about 0.5 F-score on both Chinese datasets.
The max-margin loss has one extra hyper-parameter，namely the margin value，which is set to 1 according to preliminary results on English and not tuned on Chinese for simplicity.
We suspect that the performance on Chinese with max-margin loss may be improved with more tuning.
%First，we compare the one-stage CRF approach，with widely used max-margin loss.
%The results of max-margin are very close to that of CRF in English.
%But on Chinese，CRF outperforms the max-margin consistently by about 0.5.
%This is partly because we do not tune hyper-parameters for max-margin specifically.
%We believe that max-margin still has the potential to catch up with our CRF approach with a proper setting.
% the CRF parsing span and label together is named as ``CRF w/ label''. its architecture is identical to ``Max-margin'' but loss function.
Overall，we can conclude that the two training loss settings achieve very close performance，and CRF has an extra advantage of probabilistic modeling.

Comparing the second and third rows，the two CRF parsers achieve nearly the same performance on CTB5.1 and the two-stage parser achieves modest improvement over the one-stage parser by about 0.2 F-score on both PTB and CTB7.
Therefore，we can conclude that our proposed two-stage parsing approach is superior in simplicity and efficiency (see Table~\ref{table:speed}) without hurting performance.

\input{tables/con-speed.tex}
\input{tables/con-test.tex}

\subsection{dev数据上的消融实验}

To gain insights into the contributions of individual components in our proposed framework,
we then conduct the ablation study by undoing one component at a time. % to \cite{stern-etal-2017-minimal}.
Results are shown in the bottom four rows of Table~\ref{table:con-dev}.

\noindent\textbf{MBR解码的影响.}
By default，we employ CKY decoding over marginal probabilities，a.k.a. MBR decoding.
The ``w/o MBR'' row presents the results of performing decoding over span scores.
Such comparison is very interesting since it is usually assumed that MBR decoding is theoretically superior to vanilla decoding.
However，the results clearly show that
the two decoding methods achieve nearly identical performance.

\noindent\textbf{打分架构的影响.}
In order to measure the effectiveness of our new scoring architecture，we revert the biaffine scorers to the ``minus features'' method adopted by \cite{stern-etal-2017-minimal} (refer to Equation~\ref{eq:minus-score}).
It is clear that our proposed scoring method is superior to the widely used minus-feature method，and
achieves a consistent and substantial improvement of about 0.5 F-score on all three datasets.

\noindent\textbf{Dropout策略的影响.}
We keep other model settings unchanged and only replace the dropout strategy borrowed from \cite{Timothy-d17-biaffine} with the vanilla dropout strategy adopted by \cite{stern-etal-2017-minimal}.
This leads to a very large and consistent performance drop of 0.96，1.39 and 1.59 in F-score on the three datasets，respectively.
\cite{kitaev-klein-2018-constituency} replaced 双向LSTMs with a self-attention encoder in \cite{stern-etal-2017-minimal} and achieved a large improvement of 1.0 F-score by separating content and position attention.
Similarly，this work shows that the 双向LSTM-based parser can be very competitive with proper parameter settings.

\subsection{速度比较}
Table~\ref{table:speed} compares different parsing models in terms of parsing speed.
Our models are both run on a machine with Intel Xeon E5-2650 v4 CPU and Nvidia GeForce GTX 1080 Ti GPU.
Berkeley Parser and ZPar are two representative non-neural parsers without access to GPU.
\cite{stern-etal-2017-minimal} employ max-margin training and perform CKY-like decoding on CPUs.
\cite{kitaev-klein-2018-constituency} use a self-attention encoder and perform decoding using Cython for acceleration.


We can see that our one-stage CRF parser is much more efficient than previous parsers by directly performing decoding on GPU.
Our two-stage parser can parse 1,092 sentences per sentence，which is three times faster than \cite{kitaev-klein-2018-constituency}.
Of course，it is noteworthy that those parsers \cite{stern-etal-2017-minimal,kitaev-klein-2018-constituency} may be equally efficient by adopting our batchifying techniques.

The parser of \cite{gomez-rodriguez-vilares-2018-constituent} is also very efficient by treating parsing as a sequence labeling task. However，the parsing performance is much lower，as shown in Table~\ref{table:test}.

The two-stage parser is only about 10\% faster than the one-stage counterpart. The gap seems small considering the significant difference in time complexity as discussed (see Section~\ref{sub@sec:model-definition}).
The reason is that the two parsers share the same encoding and scoring components，which consume a large portion of the parsing time.

Using MBR decoding requires an extra run of the inside and back-propagation algorithms for computing marginal probabilities，and thus is less efficient.
As shown in Table~\ref{table:con-dev}，the performance gap is very slight between w/ and w/o MBR.

\subsection{Test数据上的结果和比较}
%The results of English and Chinese on the test data is shown in
Table~\ref{table:test} shows the final results on the test datasets under two settings，i.e.，w/o and w/ ELMo/BERT.

Most previous works do not use pretrained word embedding but use randomly initialized ones instead，except for \cite{zhou-zhao-2019-head}，who use Glove for English and structured skip-gram embeddings.
For pretrained word embeddings，we use Glove (100d) %\cite{pennington-etal-2014-glove}
for English PTB\footnote{\url{https://nlp.stanford.edu/projects/glove}},
and adopt the embeddings of \cite{li-etal-2019-attentive} trained on Gigaword 3rd Edition for Chinese.
% Looking at the results of using pretrained word embeddings,
It is clear that our parser benefits substantially from the pretrained word embeddings.\footnote{
    We have also tried the structured skip-gram embeddings kindly shared by \cite{zhou-zhao-2019-head} for Chinese，and achieved similar performance by using our own embeddings.
}

We also make comparisons with recent related works on constituency parsing，as discussed in Section~\ref{sec:relwork}.
We can see that our 双向LSTM-based parser outperforms the basic \cite{stern-etal-2017-minimal} by a very large margin，mostly owing to the new scoring architecture and better dropout settings.
Compared with the previous state-of-the-art self-attentive parser \cite{kitaev-klein-2018-constituency},
our parser achieves an absolute improvement of 0.16 on PTB and 1.67 on CTB5.1 without any language-specific settings.

The CTB5.1 results of \cite{zhou-zhao-2019-head} is obtained by rerunning their released code using predicted POS tags.
We follow their descriptions\footnote{\url{https://github.com/DoodleJZ/HPSG-Neural-Parser}} to produce the POS tags.
% predicted by the Stanford tagger \cite{toutanova-etal-2003-feature}.
It is noteworthy that their reported results accidentally use gold POS tags on CTB5.1，which is confirmed after several turns of email communication. We are grateful for their patience and help.
We reran their released code using gold POS tags，and got 92.14 in F-score on CTB5-test，very close to the results reported in their paper.
Our parser achieves 92.66 F-score with gold POS tags.
Another detail about their paper should be clarified: for dependency parsing on Chinese，they adopt two different data split settings，both using Stanford dependencies 3.3.0 and gold POS tags.

The bottom three rows list the results under the setting of using ELMo/BERT.
We use bert-large-cased\footnote{\url{https://github.com/huggingface/transformers}} (24 layers，1024 dimensions，16 heads) for PTB following \cite{kitaev-etal-2019-multilingual}，and bert-base-chinese (12 layers，768 dimensions，12 heads) for CTB.
It is clear that using BERT representations can help our parser by a very large margin on all datasets. %，which is consistent with previous works.
Our parser also outperforms the multilingual parser of \cite{kitaev-etal-2019-multilingual}，which uses extra multilingual resources.
In summary，we can conclude that our parser achieves state-of-the-art performance in both languages and both settings.

\section{本章小结}\label{sec:con-conclusions}

In this work，we propose a fast and accurate neural CRF constituency parser. We show that the inside and CKY算法s can be effectively batchified to accommodate direct large tensor computation on GPU，leading to dramatic efficiency improvement.
The back-propagation procedure is equally efficient and erases the need for the Outside算法 for gradient computation.
Experiments on three English and Chinese benchmark datasets lead to several promising findings.
First，the simple two-stage bracketing-then-labeling approach is more efficient than one-stage parsing without hurting performance.
Second，our new scoring architecture achieves higher performance than the previous method based on minus features.
Third，the dropout strategy we introduce can improve parsing performance by a large margin.
Finally，our proposed parser achieves new state-of-the-art performances with a parsing speed of over 1,000 sentences per second.
