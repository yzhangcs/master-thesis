\chapter{高效的基于变分推断的高阶依存句法分析}\label{cha:approximate-vi}

本章节提出了一个采用消息传递机制的二阶基于图的和端到端神经网络模型.
这里我们从经验上表明，我们采用的近似方法不仅从准确率上可以与章节~\ref{cha:dep-crf}里提出的二阶基于图的解析器相匹敌，并且其训练和测试的速度都要远远快于二阶CRF解析器.

\section{引言}\label{sec:vi-intro}
基于图的依存句法分析一直以来都是依存句法分析领域的一个比较流行的方法.
在本章节中，我们在章节~\ref{cha:dep-crf}的基础上继续探索基于图的依存句法分析，即给定一个句子，选择将句子分解为多个部分并对他们打分，进而选择分值最高的句法树.

一阶基于图的依存句法分析将一棵完整的句法树分解为多条弧，将每条弧都视为一个独立的部分.
在深度学习时代之前，已经有大量的方法被提出用于一阶依存句法分析\cite{mcdonald-pereira-2006-online,koo-etal-2007-structured,ma-hovy-2017-neural,dozat-etal-2017-biaffine}.

这些方法通常依赖于大量的手工特征的设计，以及结构化学习方法，比如Max Margin、TreeCRF和Matrix Theorem等等，这些方法通常显示建模树的约束，更新参数使得模型尽可能预测出正确的树结构.
得益于深度神经网络的强大上下文建模能力，近期的一些工作通常基于更简单的训练方法.
其中\cite{dozat-etal-2017-biaffine}（Biaffine Parser）提出一个简单的基于头选择目标的解析器，训练时目的是最大化每条弧的头的概率.
他们采用了深度双向LSTM网络作为编码器，并使用了一个双仿射结构的打分器来给依存弧打分.
由于其高效率，并且取得了不逊色于结构化学习方法的结果\cite{zhang-etal-2019-empirical,falenska-kuhn-2019-non}，Biaffine Parser是目前最为流行的依存解析器.

与此对应的，

在神经网络时代之前，结构化学习被证明对于
已经有大量的精确推断方法\cite{mcdonald-pereira-2006-online,carreras-2007-experiments,koo-collins-2010-efficient,ma-zhao-2012-fourth}应用于句法分析，旨在找出分值最高的句法树.
近期的一些基于图的工作关注于神经网络方法\cite{chen-manning-2014-fast,kiperwasser-goldberg-2016-simple,dozat-etal-2017-biaffine,ma-hovy-2017-neural}.

后续有工作进一步引入了二阶的推断算法.
\cite{ji-etal-2019-graph}提出利用图神经网络来捕获词的二阶信息，从而进行一阶解析.
\cite{fonseca-martins-2020-revisiting}回顾了可以应用于神经网络模型的高阶解析方法，他们在打分的时候引入了二阶项，然后使用Max Margin方法训练，最大化正确树和其他树的分值边际.
我们在章节~\ref{cha:dep-crf}则提出了一个利用高效的二阶树形条件随机场进行精确推断的模型，并达到了当前最佳的解析器水平.

而高阶依存句法分析的考虑更加复杂，一棵树的分解会包含有多条边的子树.
近似方法\cite{smith-eisner-2008-dependency,gormley-etal-2015-approximation}，并使用$AD^3$\cite{martins-etal-2011-dual,martins-etal-2013-turning}来进行解码.

\section{方法}\label{sec:vi-approach}

我们的基本模型架构以\cite{dozat-etal-2017-biaffine}（Biaffine Parser）为基础，但是采用了不同的训练目标.
\cite{dozat-etal-2017-biaffine}在训练时搜索每个词对应的概率最大的头，也就是头选择目标\cite{zhang-etal-2017-head}.
而我们的模型则将依存句法分析视为二分类任务，即训练时预测每个词对对应的弧存在或者不存在.
这种方法和\cite{dozat-etal-2017-biaffine}的主要区别在于去除了单一头的约束.
\cite{eisner-1996-three}最早将这个训练目标应用到了依存句法分析任务中.
\cite{smith-eisner-2008-dependency}后来将其应用到了基于循环置信推断的近似算法中.
因此我们在Biaffine Parser的基础上换用了这种训练方法.
和\cite{zhang-etal-2019-empirical}一样，我们比较了头选择和二分类这两种基于不同归一化方法的模型，最终发现效果差别不大，见章节~\ref{sec:vi-exp}.

具体而言，给定一个句子$\boldsymbol{x}$，模型使用双向LSTM来计算上下文表示，然后将上下文表示分别输入到两个不同的模块进行两阶段解析.
第一阶段模型预测所有的词对$w_i,w_j$对应的弧$i\rightarrow j$是否存在，然后输入到MLP层和仿射层来为一阶结构和二阶结构打分.
第二阶段则在第一阶段的基础上预测存在弧的依存关系，这里和Biaffine Parser采用的方法完全一致.

\subsection{打分方法}

\begin{figure}[tb!]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[scale=1]{figures/dep-factors.pdf}
        \caption{依存句法模型的因子图}
        \label{fig:dep-factors}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[scale=1]{figures/con-factors.pdf}
        \caption{成分句法模型的因子图}
        \label{fig:con-factors}
    \end{subfigure}
    \caption{一个例句和其依存/成分句法模型对应的因子图，我们在例句上方标出了对应的正确无标签依存句法树和成分句法树，其中成分句法树进行了左二叉化.
        灰色虚线圆圈代表被屏蔽的变量.
        图中标出了所有的一阶（红色）因子，为了简洁起见，对于二阶因子，依存句法图中我们只标出了涉及弧$3\rightarrow 4$的兄弟（绿色）因子，成分句法图中只标出了和组块$(3, 4)$连接的二阶（蓝色）因子.}
    \label{fig:vi-factors}
\end{figure}

\subsection{变分推断}

在得到分值之后，我们的句法分析方法分为两阶段：1）构建无标签树；2）预测标签.
在第二阶段预测标签的时候，我们以贪婪的方式给依存树的每条弧（见章节~\ref{sub@sec:dep-crf-labeling}）或者成分树的每个组块（见章节~\ref{sub@sec:con-crf-model-definition}）打上标签.
而在第一阶段构建无标签树时，我们的目的是选择后验概率最大的句法树$\hat{\boldsymbol{y}} = \arg\max_{\boldsymbol{y}} P(\boldsymbol{y}\mid \boldsymbol{x})$.
通常这样的推断的复杂度较高，以至于无法计算.
在句法分析领域我们可以应用复杂度为$O(n^3)$的Inside算法来进行精确计算，尽管在前面章节我们展示了可以通过批次化以及张量并行计算来降低复杂度，但是仍然十分影响计算效率.
因此在本节我们提出利用基于平均场理论的变分推断法（Mean Field Variational Inference, MFVI）来近似得到后验概率.
MFVI假设句法树$\boldsymbol{y}$每个位置的变量相互独立，因此可以在线性时间内通过迭代的方法得到后验概率的近似值$Q(\boldsymbol{y})$.

关于MFVI的通用更新公式以及相关推导见附录~\ref{appendix:mfvi-derivation}.
根据依存句法和成分句法任务目标的不同，我们在下面两小节详细阐述了通用公式针对具体任务的特化版本，以及相关的势函数（potential function）和因子图（factor graph）的设计.

\noindent\textbf{基于头选择的变分推断.}
目前广泛使用的一阶局部模型（对应于章节~\ref{cha:dep-crf}的\textsc{Loc}方法，这里我们称为\textsc{Loc}$^{dep}$）在训练的时候采用了头选择（head selection）训练目标，要求句子中除了根结点之外的每个词有且仅有一个头.
和\cite{wang-tu-2020-second}一样，我们选择在依存句法对应变分推断方法中引入头选择约束，相应的因子图见图~\ref{fig:dep-factors}..
对于每个变量$j$，我们定义变量取值为$\boldsymbol{y}_j\in \{0,1,\cdots,i\neq j,\cdots,n\}$，代表词$w_j$的可能的头索引.

具体地，对于每个变量$\boldsymbol{y}_j$，一阶的势函数定义为
\begin{equation}
    \label{eq:1o-potential}
    \psi_j(\boldsymbol{y}_j)=\exp(s(\boldsymbol{y}_j\rightarrow j))
\end{equation}
$s(\boldsymbol{y}_j\rightarrow j)$是弧$\boldsymbol{y}_j\rightarrow j$对应的分值，由公式~\ref{eq:biaffine}计算得到.

对于两个变量$\boldsymbol{y}_{j}$和$\boldsymbol{y}_{k}$，我们使用和章节~\ref{cha:dep-crf}一致的二阶特征，对应的二阶势函数定义为
\begin{equation}
    \label{eq:2o-dep-potential}
    \psi_{j,k}(\boldsymbol{y}_j,\boldsymbol{y}_k)=\left\{
    \begin{array}{rcl}
        \exp \mathrm{s}(\boldsymbol{y}_j\rightarrow \{k,j\}) &  & {\boldsymbol{y}_j=\boldsymbol{y}_k} \\
        1                                                    &  & {otherwise}
    \end{array}
    \right.
\end{equation}
$s(\boldsymbol{y}_j\rightarrow \{k,j\})$是兄弟子树$\boldsymbol{y}_j\rightarrow \{k,j\}$的分值，由公式~\ref{eq:triaffine}计算得到.
需要注意的是这里的兄弟$k$和二阶TreeCRF不一样，并不需要一定和$j$邻近.

MFVI迭代式地更新近似分布$Q(\cdot)$，来最小化其与真实分布$P(\cdot)$的KL散度，依存句法模型的迭代更新公式如下\cite{wang-tu-2020-second}：
\begin{equation}
    \label{eq:mfvi-dep}
    Q_{j}^{(t)}(i)\propto \exp\left(s(i\rightarrow j) +\sum_{k\neq i,j} Q_{k}^{(t-1)}(i)\cdot s(i\rightarrow {k,j}) \right)
\end{equation}
后验概率$Q_j^{(0)}(i)$初始化为一阶势函数$\psi_j(i)$.
每次迭代时，我们都会将$Q_j^{(0)}(\cdot)$的值在所有可能的头取值上进行归一化.

\noindent\textbf{基于二分类的变分推断.}
我们的成分句法分析模型采用了\cite{gaddy-etal-2018-whats}的方法作为基线方法，称为\textsc{Loc}$^{con}$.
具体来说，模型对短语树所有可能的位置进行一个简单的二分类，预测该位置是否是一个区块.
\cite{dozat-manning-2018-simpler,wang-etal-2019-second}在语义依存分析中应用了这样的训练目标.
\cite{gormley-eisner-2015-structured,naradowsky-etal-2012-grammarless}在成分句法分析中应用了这种方法，并采用循环置信传播（Loopy Belief Propagation, LBP）来近似获得后验概率，并设计了若干种局部和全局因子，比如\textsc{Exactly1}、\textsc{Tree}等.
我们将其引入到了基于MFVI的成分句法分析中，但是简单起见仅仅采用了一个一阶因子和一个二阶因子，对应的因子图见图~\ref{fig:con-factors}.
我们考虑将引入更多因子约束的近似算法留待作为未来工作.


具体地，每个位置$ij$的可能的变量取值$\boldsymbol{y}_{ij}\in \{0,1\}$. 对于单个变量$\boldsymbol{y}_{ij}$，一阶的势函数定义为
\begin{equation}
    \label{eq:1o-potential}
    \psi_{ij}(\boldsymbol{y}_{ij})=\left\{
    \begin{array}{rcl}
        \exp\left(\mathrm{s}(i,j)\right) &  & {\boldsymbol{y}_{ij}=1} \\
        1                                &  & {otherwise}
    \end{array}
    \right.
\end{equation}
$s(i,j)$是区块$(i,j)$的分值，由公式~\ref{eq:con-biaffine}计算得到.

给定两个变量$\boldsymbol{y}_{ij}$和$\boldsymbol{y}_{lk}$，我们考虑使用和章节~\ref{cha:con-crf}一致的二阶兄弟特征，因此二阶的势函数定义为
\begin{equation}
    \label{eq:2o-con-potential}
    \psi_{ij,lk}(\boldsymbol{y}_{ij},\boldsymbol{y}_{lk})=\left\{
    \begin{array}{rcl}
        \exp\left(\mathrm{s}(i,k,j)\right) &  & {i=l}       \\
        1                                  &  & {otherwise}
    \end{array}
    \right.
\end{equation}
$s(i,k,j)$可以视为$(i,j)$和$(i,k)$都作为组块时，组成的子树的分值，由公式~\ref{eq:con-triaffine}计算得到，这里$k$的位置不受动态规划算法的约束，并不要求一定位于$(i，j)$之间.

成分句法模型的MFVI迭代更新公式如下\cite{wang-etal-2019-second}：
\begin{equation}
    \label{eq:mfvi-con}
    \begin{array}{l}
        Q_{ij}^{(t)}(0)\propto 1 \\
        Q_{ij}^{(t)}(1)\propto \exp\left(s(i,j) +\sum_{k\neq i,j} Q_{ik}^{(t-1)}(1)\cdot s(i,k,j) \right)
    \end{array}
\end{equation}
后验概率$Q_{ij}^{(0)}(\boldsymbol{y}_{ij})$初始化为一阶势函数$\psi_{ij}(\boldsymbol{y}_{ij})$.
每次迭代时，我们将$Q_{ij}^{(0)}(\cdot)$在取值$\{0,1\}$上进行归一化.


\subsection{训练}

\subsection{解码}

\subsection{复杂度分析}


\begin{figure}[tb]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[scale=0.75]{figures/dep-probs.pdf}
        \caption{成分句法树每个位置对应的分值和后验概率}
        \label{fig:dep-probs}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[scale=0.75]{figures/con-probs.pdf}
        \caption{成分句法树每个位置对应的分值和后验概率}
        \label{fig:con-probs}
    \end{subfigure}
    \caption{一个例句对应的依存句法模型和成分句法模型的输出. 左边的图是每个位置的分值（log potential），为了方便显示，我们首先对分值进行了归一化.
        右边的图是变分推断得到的后验概率. 灰色虚线圆圈代表被屏蔽的不合法位置，颜色的深浅反应了分值/概率的大小.}

    \label{fig:vi-probs}
\end{figure}

\section{实验}\label{sec:vi-exp}

\noindent\textbf{数据.}
为了方面和章节~\ref{cha:dep-crf}提出的精确推断的高阶模型进行比较，我们主要在英文的PTB和中文的CNLL09上进行了依存句法的实验.
同样的，我们在英文的PTB，中文的CTB51以及CTB7进行了成分句法分析实验.
实验数据的详细设置在章节~\ref{cha:dep-crf}和章节~\ref{cha:con-crf}有详细的设置，这里简洁起见不再重复.

\noindent\textbf{评价.}
在依存句法分析上，我们主要采用了有标签和无标签附着分值（UAS/LAS）作为评价指标，英文PTB的标点被忽略.
在成分句法分析上，我们采用了惯用的区块级别的准确率、召回率和F值（P/R/F）的指标，由标准工具\texttt{EVALB}来得到.
成分句法树在训练和评价时采用的预处理和后处理行为我们保持和章节~\ref{cha:con-crf}一致.

\noindent\textbf{参数设置.}
我们保持两种句法分析模型的编码器和训练方法与前面的章节基本一致.
对于二阶模型，我们设置依存句法分析中使用的兄弟特征以及成分句法分析使用的二阶特征的MLP层输出维度为100.
我们设置变分推断的迭代次数统一为3次.
在成分句法分析中我们新加入了一个参数$\lambda$来平衡标签和无标签树的训练损失，并设置$\lambda$为0.1.

\input{tables/dep-vi-dev.tex}

\subsection{分析}

\noindent\textbf{子树和完全树的结果.}

\noindent\textbf{数据量的影响.}

\noindent\textbf{迭代次数的影响.}

\subsection{样例分析}

\subsection{速度比较}

\subsection{UD的结果}
